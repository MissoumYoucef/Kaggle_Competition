{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNTrxzld9XNXgZajurAH++G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MissoumYoucef/Kaggle_Competition/blob/main/Facial_Keypoints_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Conv2D,Dropout,Dense,Flatten\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Activation, Convolution2D, MaxPooling2D, BatchNormalization, Flatten, Dense, Dropout, Conv2D,MaxPool2D, ZeroPadding2D\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "metadata": {
        "id": "Z7lZb98F6Tp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_csv('training.csv')\n",
        "test_data = pd.read_csv('test.csv')"
      ],
      "metadata": {
        "id": "tzQ-XE7SJRHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.shape,test_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPJfUv_lJTJ2",
        "outputId": "721d17db-06f8-41e4-ed1d-27f390732735"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((7049, 31), (1783, 2))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3j10HHsPJ8o2",
        "outputId": "1b2194c9-d64f-4587-85dc-dd7ecba517c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['left_eye_center_x', 'left_eye_center_y', 'right_eye_center_x',\n",
              "       'right_eye_center_y', 'left_eye_inner_corner_x',\n",
              "       'left_eye_inner_corner_y', 'left_eye_outer_corner_x',\n",
              "       'left_eye_outer_corner_y', 'right_eye_inner_corner_x',\n",
              "       'right_eye_inner_corner_y', 'right_eye_outer_corner_x',\n",
              "       'right_eye_outer_corner_y', 'left_eyebrow_inner_end_x',\n",
              "       'left_eyebrow_inner_end_y', 'left_eyebrow_outer_end_x',\n",
              "       'left_eyebrow_outer_end_y', 'right_eyebrow_inner_end_x',\n",
              "       'right_eyebrow_inner_end_y', 'right_eyebrow_outer_end_x',\n",
              "       'right_eyebrow_outer_end_y', 'nose_tip_x', 'nose_tip_y',\n",
              "       'mouth_left_corner_x', 'mouth_left_corner_y', 'mouth_right_corner_x',\n",
              "       'mouth_right_corner_y', 'mouth_center_top_lip_x',\n",
              "       'mouth_center_top_lip_y', 'mouth_center_bottom_lip_x',\n",
              "       'mouth_center_bottom_lip_y', 'Image'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the image data and keypoint coordinates\n",
        "images = train_data['Image'].apply(lambda x: np.array(x.split(' '), dtype=int))"
      ],
      "metadata": {
        "id": "a8APX-7q6OfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "null_counts=train_data.isna().sum()\n",
        "# null_counts"
      ],
      "metadata": {
        "id": "7TU5JDlWQBJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "non_much_Null_values=null_counts.index\n",
        "non_much_Null_values = [col for col in non_much_Null_values if col != 'Image']\n",
        "# non_much_Null_values"
      ],
      "metadata": {
        "id": "FqcVlpCLTvO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# # Assuming your DataFrame is named 'df'\n",
        "# for feature in non_much_Null_values:\n",
        "#     train_data[feature].hist(bins=30)\n",
        "#     plt.xlabel(feature)\n",
        "#     plt.ylabel('Frequency')\n",
        "#     plt.title('Distribution of {}'.format(feature))\n",
        "#     plt.show()"
      ],
      "metadata": {
        "id": "JAu83x4-QBEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for feature in non_much_Null_values:\n",
        "    median_value = train_data[feature].median()\n",
        "    train_data[feature].fillna(median_value, inplace=True)"
      ],
      "metadata": {
        "id": "FsKtzYDSLfqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keypoints = train_data.drop('Image', axis=1)"
      ],
      "metadata": {
        "id": "vVBZ3osyV9bM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(images, keypoints, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "boQUiIqlM0vn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape the image data\n",
        "X_train = np.array([x.reshape(96, 96, 1) for x in X_train])\n",
        "X_val = np.array([x.reshape(96, 96, 1) for x in X_val])"
      ],
      "metadata": {
        "id": "mi4KWeeJ6Rx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.applications import EfficientNetV2S\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "PAFNxIAE2lHD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to convert grayscale images to RGB\n",
        "def gray_to_rgb(images):\n",
        "    return np.stack([images] * 3, axis=-1)"
      ],
      "metadata": {
        "id": "Tf89ljKz6Oad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Resize images to 260x260 and convert to RGB\n",
        "images_resized = tf.image.resize(train_data['image'], [260, 260])\n",
        "images_rgb = gray_to_rgb(images_resized)"
      ],
      "metadata": {
        "id": "vRSomsxT2luz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = EfficientNetV2S(weights='imagenet', include_top=False, input_shape=(260, 260, 3))\n",
        "base_model.trainable = False  # Freeze the base model"
      ],
      "metadata": {
        "id": "Boj1hRMP237R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.Sequential([\n",
        "    base_model,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.Dropout(0.1),\n",
        "    layers.Dense(30)  # Assuming a binary classification problem\n",
        "])"
      ],
      "metadata": {
        "id": "cuupuRDN29AV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = Sequential()\n",
        "\n",
        "# # model.add(Convolution2D(32, (3,3), padding='same', use_bias=False, input_shape=(96,96,1)))\n",
        "# # model.add(LeakyReLU(alpha = 0.1))\n",
        "# # model.add(BatchNormalization())\n",
        "\n",
        "# # model.add(Convolution2D(32, (3,3), padding='same', use_bias=False))\n",
        "# # model.add(LeakyReLU(alpha = 0.1))\n",
        "# # model.add(BatchNormalization())\n",
        "# # model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "# # # model.add(Convolution2D(64, (3,3), padding='same', use_bias=False))\n",
        "# # # model.add(LeakyReLU(alpha = 0.1))\n",
        "# # # model.add(BatchNormalization())\n",
        "\n",
        "# # # model.add(Convolution2D(64, (3,3), padding='same', use_bias=False))\n",
        "# # # model.add(LeakyReLU(alpha = 0.1))\n",
        "# # # model.add(BatchNormalization())\n",
        "# # # model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "# # # model.add(Convolution2D(96, (3,3), padding='same', use_bias=False))\n",
        "# # # model.add(LeakyReLU(alpha = 0.1))\n",
        "# # # model.add(BatchNormalization())\n",
        "\n",
        "# # # model.add(Convolution2D(96, (3,3), padding='same', use_bias=False))\n",
        "# # # model.add(LeakyReLU(alpha = 0.1))\n",
        "# # # model.add(BatchNormalization())\n",
        "# # # model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "# # # model.add(Convolution2D(128, (3,3),padding='same', use_bias=False))\n",
        "# # # # model.add(BatchNormalization())\n",
        "# # # model.add(LeakyReLU(alpha = 0.1))\n",
        "# # # model.add(BatchNormalization())\n",
        "\n",
        "# # # model.add(Convolution2D(128, (3,3),padding='same', use_bias=False))\n",
        "# # # model.add(LeakyReLU(alpha = 0.1))\n",
        "# # # model.add(BatchNormalization())\n",
        "# # # model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "# # # model.add(Convolution2D(256, (3,3),padding='same',use_bias=False))\n",
        "# # # model.add(LeakyReLU(alpha = 0.1))\n",
        "# # # model.add(BatchNormalization())\n",
        "\n",
        "# # # model.add(Convolution2D(256, (3,3),padding='same',use_bias=False))\n",
        "# # # model.add(LeakyReLU(alpha = 0.1))\n",
        "# # # model.add(BatchNormalization())\n",
        "# # # model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "# # # model.add(Convolution2D(512, (3,3), padding='same', use_bias=False))\n",
        "# # # model.add(LeakyReLU(alpha = 0.1))\n",
        "# # # model.add(BatchNormalization())\n",
        "\n",
        "# # model.add(Convolution2D(512, (3,3), padding='same', use_bias=False))\n",
        "# # model.add(LeakyReLU(alpha = 0.1))\n",
        "# # model.add(BatchNormalization())\n",
        "\n",
        "\n",
        "# model.add(Flatten())\n",
        "# model.add(Dense(512,activation='relu'))\n",
        "# model.add(Dropout(0.1))\n",
        "# model.add(Dense(30))\n",
        "# model.summary()"
      ],
      "metadata": {
        "id": "7A1u98FqaA1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='mean_squared_error',\n",
        "              metrics=['mae'])"
      ],
      "metadata": {
        "id": "kSML9H93a1BZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train,y_train,epochs = 5,batch_size = 256,validation_split = 0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNWSdfOxa2DS",
        "outputId": "8ffef07e-e5bf-4897-c1bf-bc79e4b7886b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "18/18 [==============================] - 231s 12s/step - loss: 520.2770 - mae: 16.2001 - val_loss: 2497.5452 - val_mae: 46.4705\n",
            "Epoch 2/5\n",
            "18/18 [==============================] - 211s 12s/step - loss: 93.2856 - mae: 7.5849 - val_loss: 2638.5972 - val_mae: 46.5951\n",
            "Epoch 3/5\n",
            "18/18 [==============================] - 217s 12s/step - loss: 57.7662 - mae: 5.9851 - val_loss: 2942.7329 - val_mae: 47.9032\n",
            "Epoch 4/5\n",
            "18/18 [==============================] - 210s 12s/step - loss: 45.5867 - mae: 5.3031 - val_loss: 3343.0193 - val_mae: 49.9338\n",
            "Epoch 5/5\n",
            "18/18 [==============================] - 217s 12s/step - loss: 41.1386 - mae: 5.0294 - val_loss: 3690.7939 - val_mae: 51.4732\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a875e8746d0>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#preparing test data\n",
        "timag = []\n",
        "for i in range(0,1783):\n",
        "    timg = test_data['Image'][i].split(' ')\n",
        "    timg = ['0' if x == '' else x for x in timg]\n",
        "\n",
        "    timag.append(timg)"
      ],
      "metadata": {
        "id": "KTFFr14cbaiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "timage_list = np.array(timag,dtype = 'float')\n",
        "X_test = timage_list.reshape(-1,96,96,1)"
      ],
      "metadata": {
        "id": "IivHrqGNcEoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = model.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8ru8xUha9MD",
        "outputId": "2a167c25-a65b-4564-fd61-9850a2749342"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "56/56 [==============================] - 19s 338ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lookid_data = pd.read_csv('IdLookupTable.csv')\n",
        "lookid_list = list(lookid_data['FeatureName'])\n",
        "imageID = list(lookid_data['ImageId']-1)\n",
        "pre_list = list(pred)\n",
        "rowid = lookid_data['RowId']\n",
        "rowid=list(rowid)\n",
        "feature = []\n",
        "for f in list(lookid_data['FeatureName']):\n",
        "    feature.append(lookid_list.index(f))\n",
        "preded = []\n",
        "for x,y in zip(imageID,feature):\n",
        "    preded.append(pre_list[x][y])\n",
        "rowid = pd.Series(rowid,name = 'RowId')\n",
        "loc = pd.Series(preded,name = 'Location')\n",
        "submission = pd.concat([rowid,loc],axis = 1)\n",
        "submission.to_csv('face_key_detection_submission.csv',index = False)"
      ],
      "metadata": {
        "id": "0RibNmMn6ZxW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}