{
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "TweetsDisasterWithbert.ipynb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 17777,
          "databundleVersionId": 869809,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 30673,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MissoumYoucef/Kaggle_Competition/blob/main/TweetsDisasterWithbert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'nlp-getting-started:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-competitions-data%2Fkaggle-v2%2F17777%2F869809%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240406%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240406T071055Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D70ec557af455f863fb6e2bd6759c217cdc00699b39c13d21de4e1716e015997d5ab844ea74ba440600c20e978ca301ccddd60ed7b62f3d69a71a7acd3c83131274bc96b392da0a60ad35589202e634892654f5f94d5afff5b8694735bd466be7b6c21f2e667b4b786803fb6d6ce30a034e559560c71058a3fcaaf830f128fe8577128a169a2af5102b30395a29dc1da98d6a60e203b44b4fbbfc6bcb45f58af3fbc29380cee33fb66064c0283f72a81f69722103afaad1582314dd314f62445654795bf584d478b19b929bc3275fb1aacf85612cb07411db9335aa97aabde8537d6e4092b849ebb9ebcc0b83570edefb10ab06d1a87de176993af8cb3f66bdd9'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "KiVxCQonPuiL"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-06T02:47:46.356082Z",
          "iopub.execute_input": "2024-04-06T02:47:46.356739Z",
          "iopub.status.idle": "2024-04-06T02:47:46.824634Z",
          "shell.execute_reply.started": "2024-04-06T02:47:46.356677Z",
          "shell.execute_reply": "2024-04-06T02:47:46.823781Z"
        },
        "trusted": true,
        "id": "qMyiCFxCPuiN",
        "outputId": "3ab0f00b-0d52-4dc0-a6b8-d2c6c75488e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "/kaggle/input/nlp-getting-started/sample_submission.csv\n/kaggle/input/nlp-getting-started/train.csv\n/kaggle/input/nlp-getting-started/test.csv\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from gensim.models import KeyedVectors\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import f1_score\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "import spacy\n",
        "import re\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-06T02:47:46.826741Z",
          "iopub.execute_input": "2024-04-06T02:47:46.827493Z",
          "iopub.status.idle": "2024-04-06T02:48:23.064838Z",
          "shell.execute_reply.started": "2024-04-06T02:47:46.827457Z",
          "shell.execute_reply": "2024-04-06T02:48:23.063668Z"
        },
        "trusted": true,
        "id": "VOIYMU70PuiO",
        "outputId": "ca1b1be4-faf6-4872-d6fb-4904c1f83567"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "2024-04-06 02:47:47.251554: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-06 02:47:47.251658: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-06 02:47:47.253787: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n",
          "output_type": "stream"
        },
        {
          "execution_count": 2,
          "output_type": "execute_result",
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\n",
        "test = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')"
      ],
      "metadata": {
        "id": "aAIb35HKcJqU",
        "execution": {
          "iopub.status.busy": "2024-04-06T02:48:23.066213Z",
          "iopub.execute_input": "2024-04-06T02:48:23.066946Z",
          "iopub.status.idle": "2024-04-06T02:48:23.133818Z",
          "shell.execute_reply.started": "2024-04-06T02:48:23.066902Z",
          "shell.execute_reply": "2024-04-06T02:48:23.13292Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.shape"
      ],
      "metadata": {
        "id": "aeG8TY_pNuv2",
        "outputId": "a528a55a-ad95-496f-97e0-3513607a0a61",
        "execution": {
          "iopub.status.busy": "2024-04-06T02:48:23.135205Z",
          "iopub.execute_input": "2024-04-06T02:48:23.135939Z",
          "iopub.status.idle": "2024-04-06T02:48:23.145542Z",
          "shell.execute_reply.started": "2024-04-06T02:48:23.135898Z",
          "shell.execute_reply": "2024-04-06T02:48:23.143863Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 4,
          "output_type": "execute_result",
          "data": {
            "text/plain": "(7613, 5)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train.head()"
      ],
      "metadata": {
        "id": "EK-dhcsrIw1U",
        "outputId": "5cf3570a-b57d-4777-ecb8-f0d40f514a0e",
        "execution": {
          "iopub.status.busy": "2024-04-06T02:48:23.148775Z",
          "iopub.execute_input": "2024-04-06T02:48:23.149101Z",
          "iopub.status.idle": "2024-04-06T02:48:23.168947Z",
          "shell.execute_reply.started": "2024-04-06T02:48:23.149067Z",
          "shell.execute_reply": "2024-04-06T02:48:23.16776Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 5,
          "output_type": "execute_result",
          "data": {
            "text/plain": "   id keyword location                                               text  \\\n0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n\n   target  \n0       1  \n1       1  \n2       1  \n3       1  \n4       1  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def preprocess_text(text):\n",
        "#     lemmatizer = WordNetLemmatizer()\n",
        "#     # Remove URLs\n",
        "#     text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "#     # Remove mentions and hashtags\n",
        "#     text = re.sub(r'\\@w+|\\#','', text)\n",
        "#     # Remove punctuation and non-word characters\n",
        "#     text = re.sub(r'[^\\w\\s]', '', text)\n",
        "#     # Tokenize the text\n",
        "#     text_tokens = text.split()\n",
        "#     # Remove stopwords and lemmatize\n",
        "#     lemmatized_words = [lemmatizer.lemmatize(word) for word in text_tokens if word not in stopwords.words('english')]\n",
        "#     return \" \".join(lemmatized_words)"
      ],
      "metadata": {
        "id": "uUGr9Pjkb_cl",
        "execution": {
          "iopub.status.busy": "2024-04-06T02:48:23.170362Z",
          "iopub.execute_input": "2024-04-06T02:48:23.171503Z",
          "iopub.status.idle": "2024-04-06T02:48:23.176665Z",
          "shell.execute_reply.started": "2024-04-06T02:48:23.171466Z",
          "shell.execute_reply": "2024-04-06T02:48:23.175447Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train['text'] = train['text'].apply(preprocess_text)\n",
        "# test['text'] = test['text'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "VDmUr3G2b-6M",
        "execution": {
          "iopub.status.busy": "2024-04-06T02:48:23.178202Z",
          "iopub.execute_input": "2024-04-06T02:48:23.178634Z",
          "iopub.status.idle": "2024-04-06T02:48:23.188598Z",
          "shell.execute_reply.started": "2024-04-06T02:48:23.1786Z",
          "shell.execute_reply": "2024-04-06T02:48:23.187659Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "def clean(tweet):\n",
        "\n",
        "    # Special characters\n",
        "    tweet = re.sub(r\"\\x89Û_\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89ÛÒ\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89ÛÓ\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89ÛÏWhen\", \"When\", tweet)\n",
        "    tweet = re.sub(r\"\\x89ÛÏ\", \"\", tweet)\n",
        "    tweet = re.sub(r\"China\\x89Ûªs\", \"China's\", tweet)\n",
        "    tweet = re.sub(r\"let\\x89Ûªs\", \"let's\", tweet)\n",
        "    tweet = re.sub(r\"\\x89Û÷\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89Ûª\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89Û\\x9d\", \"\", tweet)\n",
        "    tweet = re.sub(r\"å_\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89Û¢\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89Û¢åÊ\", \"\", tweet)\n",
        "    tweet = re.sub(r\"fromåÊwounds\", \"from wounds\", tweet)\n",
        "    tweet = re.sub(r\"åÊ\", \"\", tweet)\n",
        "    tweet = re.sub(r\"åÈ\", \"\", tweet)\n",
        "    tweet = re.sub(r\"JapÌ_n\", \"Japan\", tweet)\n",
        "    tweet = re.sub(r\"Ì©\", \"e\", tweet)\n",
        "    tweet = re.sub(r\"å¨\", \"\", tweet)\n",
        "    tweet = re.sub(r\"SuruÌ¤\", \"Suruc\", tweet)\n",
        "    tweet = re.sub(r\"åÇ\", \"\", tweet)\n",
        "    tweet = re.sub(r\"å£3million\", \"3 million\", tweet)\n",
        "    tweet = re.sub(r\"åÀ\", \"\", tweet)\n",
        "\n",
        "    # Contractions\n",
        "    tweet = re.sub(r\"he's\", \"he is\", tweet)\n",
        "    tweet = re.sub(r\"there's\", \"there is\", tweet)\n",
        "    tweet = re.sub(r\"We're\", \"We are\", tweet)\n",
        "    tweet = re.sub(r\"That's\", \"That is\", tweet)\n",
        "    tweet = re.sub(r\"won't\", \"will not\", tweet)\n",
        "    tweet = re.sub(r\"they're\", \"they are\", tweet)\n",
        "    tweet = re.sub(r\"Can't\", \"Cannot\", tweet)\n",
        "    tweet = re.sub(r\"wasn't\", \"was not\", tweet)\n",
        "    tweet = re.sub(r\"don\\x89Ûªt\", \"do not\", tweet)\n",
        "    tweet = re.sub(r\"aren't\", \"are not\", tweet)\n",
        "    tweet = re.sub(r\"isn't\", \"is not\", tweet)\n",
        "    tweet = re.sub(r\"What's\", \"What is\", tweet)\n",
        "    tweet = re.sub(r\"haven't\", \"have not\", tweet)\n",
        "    tweet = re.sub(r\"hasn't\", \"has not\", tweet)\n",
        "    tweet = re.sub(r\"There's\", \"There is\", tweet)\n",
        "    tweet = re.sub(r\"He's\", \"He is\", tweet)\n",
        "    tweet = re.sub(r\"It's\", \"It is\", tweet)\n",
        "    tweet = re.sub(r\"You're\", \"You are\", tweet)\n",
        "    tweet = re.sub(r\"I'M\", \"I am\", tweet)\n",
        "    tweet = re.sub(r\"shouldn't\", \"should not\", tweet)\n",
        "    tweet = re.sub(r\"wouldn't\", \"would not\", tweet)\n",
        "    tweet = re.sub(r\"i'm\", \"I am\", tweet)\n",
        "    tweet = re.sub(r\"I\\x89Ûªm\", \"I am\", tweet)\n",
        "    tweet = re.sub(r\"I'm\", \"I am\", tweet)\n",
        "    tweet = re.sub(r\"Isn't\", \"is not\", tweet)\n",
        "    tweet = re.sub(r\"Here's\", \"Here is\", tweet)\n",
        "    tweet = re.sub(r\"you've\", \"you have\", tweet)\n",
        "    tweet = re.sub(r\"you\\x89Ûªve\", \"you have\", tweet)\n",
        "    tweet = re.sub(r\"we're\", \"we are\", tweet)\n",
        "    tweet = re.sub(r\"what's\", \"what is\", tweet)\n",
        "    tweet = re.sub(r\"couldn't\", \"could not\", tweet)\n",
        "    tweet = re.sub(r\"we've\", \"we have\", tweet)\n",
        "    tweet = re.sub(r\"it\\x89Ûªs\", \"it is\", tweet)\n",
        "    tweet = re.sub(r\"doesn\\x89Ûªt\", \"does not\", tweet)\n",
        "    tweet = re.sub(r\"It\\x89Ûªs\", \"It is\", tweet)\n",
        "    tweet = re.sub(r\"Here\\x89Ûªs\", \"Here is\", tweet)\n",
        "    tweet = re.sub(r\"who's\", \"who is\", tweet)\n",
        "    tweet = re.sub(r\"I\\x89Ûªve\", \"I have\", tweet)\n",
        "    tweet = re.sub(r\"y'all\", \"you all\", tweet)\n",
        "    tweet = re.sub(r\"can\\x89Ûªt\", \"cannot\", tweet)\n",
        "    tweet = re.sub(r\"would've\", \"would have\", tweet)\n",
        "    tweet = re.sub(r\"it'll\", \"it will\", tweet)\n",
        "    tweet = re.sub(r\"we'll\", \"we will\", tweet)\n",
        "    tweet = re.sub(r\"wouldn\\x89Ûªt\", \"would not\", tweet)\n",
        "    tweet = re.sub(r\"We've\", \"We have\", tweet)\n",
        "    tweet = re.sub(r\"he'll\", \"he will\", tweet)\n",
        "    tweet = re.sub(r\"Y'all\", \"You all\", tweet)\n",
        "    tweet = re.sub(r\"Weren't\", \"Were not\", tweet)\n",
        "    tweet = re.sub(r\"Didn't\", \"Did not\", tweet)\n",
        "    tweet = re.sub(r\"they'll\", \"they will\", tweet)\n",
        "    tweet = re.sub(r\"they'd\", \"they would\", tweet)\n",
        "    tweet = re.sub(r\"DON'T\", \"DO NOT\", tweet)\n",
        "    tweet = re.sub(r\"That\\x89Ûªs\", \"That is\", tweet)\n",
        "    tweet = re.sub(r\"they've\", \"they have\", tweet)\n",
        "    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n",
        "    tweet = re.sub(r\"should've\", \"should have\", tweet)\n",
        "    tweet = re.sub(r\"You\\x89Ûªre\", \"You are\", tweet)\n",
        "    tweet = re.sub(r\"where's\", \"where is\", tweet)\n",
        "    tweet = re.sub(r\"Don\\x89Ûªt\", \"Do not\", tweet)\n",
        "    tweet = re.sub(r\"we'd\", \"we would\", tweet)\n",
        "    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n",
        "    tweet = re.sub(r\"weren't\", \"were not\", tweet)\n",
        "    tweet = re.sub(r\"They're\", \"They are\", tweet)\n",
        "    tweet = re.sub(r\"Can\\x89Ûªt\", \"Cannot\", tweet)\n",
        "    tweet = re.sub(r\"you\\x89Ûªll\", \"you will\", tweet)\n",
        "    tweet = re.sub(r\"I\\x89Ûªd\", \"I would\", tweet)\n",
        "    tweet = re.sub(r\"let's\", \"let us\", tweet)\n",
        "    tweet = re.sub(r\"it's\", \"it is\", tweet)\n",
        "    tweet = re.sub(r\"can't\", \"cannot\", tweet)\n",
        "    tweet = re.sub(r\"don't\", \"do not\", tweet)\n",
        "    tweet = re.sub(r\"you're\", \"you are\", tweet)\n",
        "    tweet = re.sub(r\"i've\", \"I have\", tweet)\n",
        "    tweet = re.sub(r\"that's\", \"that is\", tweet)\n",
        "    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n",
        "    tweet = re.sub(r\"doesn't\", \"does not\", tweet)\n",
        "    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n",
        "    tweet = re.sub(r\"didn't\", \"did not\", tweet)\n",
        "    tweet = re.sub(r\"ain't\", \"am not\", tweet)\n",
        "    tweet = re.sub(r\"you'll\", \"you will\", tweet)\n",
        "    tweet = re.sub(r\"I've\", \"I have\", tweet)\n",
        "    tweet = re.sub(r\"Don't\", \"do not\", tweet)\n",
        "    tweet = re.sub(r\"I'll\", \"I will\", tweet)\n",
        "    tweet = re.sub(r\"I'd\", \"I would\", tweet)\n",
        "    tweet = re.sub(r\"Let's\", \"Let us\", tweet)\n",
        "    tweet = re.sub(r\"you'd\", \"You would\", tweet)\n",
        "    tweet = re.sub(r\"It's\", \"It is\", tweet)\n",
        "    tweet = re.sub(r\"Ain't\", \"am not\", tweet)\n",
        "    tweet = re.sub(r\"Haven't\", \"Have not\", tweet)\n",
        "    tweet = re.sub(r\"Could've\", \"Could have\", tweet)\n",
        "    tweet = re.sub(r\"youve\", \"you have\", tweet)\n",
        "    tweet = re.sub(r\"donå«t\", \"do not\", tweet)\n",
        "\n",
        "    # Character entity references\n",
        "    tweet = re.sub(r\"&gt;\", \">\", tweet)\n",
        "    tweet = re.sub(r\"&lt;\", \"<\", tweet)\n",
        "    tweet = re.sub(r\"&amp;\", \"&\", tweet)\n",
        "\n",
        "    # Typos, slang and informal abbreviations\n",
        "    tweet = re.sub(r\"w/e\", \"whatever\", tweet)\n",
        "    tweet = re.sub(r\"w/\", \"with\", tweet)\n",
        "    tweet = re.sub(r\"USAgov\", \"USA government\", tweet)\n",
        "    tweet = re.sub(r\"recentlu\", \"recently\", tweet)\n",
        "    tweet = re.sub(r\"Ph0tos\", \"Photos\", tweet)\n",
        "    tweet = re.sub(r\"amirite\", \"am I right\", tweet)\n",
        "    tweet = re.sub(r\"exp0sed\", \"exposed\", tweet)\n",
        "    tweet = re.sub(r\"<3\", \"love\", tweet)\n",
        "    tweet = re.sub(r\"amageddon\", \"armageddon\", tweet)\n",
        "    tweet = re.sub(r\"Trfc\", \"Traffic\", tweet)\n",
        "    tweet = re.sub(r\"8/5/2015\", \"2015-08-05\", tweet)\n",
        "    tweet = re.sub(r\"WindStorm\", \"Wind Storm\", tweet)\n",
        "    tweet = re.sub(r\"8/6/2015\", \"2015-08-06\", tweet)\n",
        "    tweet = re.sub(r\"10:38PM\", \"10:38 PM\", tweet)\n",
        "    tweet = re.sub(r\"10:30pm\", \"10:30 PM\", tweet)\n",
        "    tweet = re.sub(r\"16yr\", \"16 year\", tweet)\n",
        "    tweet = re.sub(r\"lmao\", \"laughing my ass off\", tweet)\n",
        "    tweet = re.sub(r\"TRAUMATISED\", \"traumatized\", tweet)\n",
        "\n",
        "    # Hashtags and usernames\n",
        "    tweet = re.sub(r\"IranDeal\", \"Iran Deal\", tweet)\n",
        "    tweet = re.sub(r\"ArianaGrande\", \"Ariana Grande\", tweet)\n",
        "    tweet = re.sub(r\"camilacabello97\", \"camila cabello\", tweet)\n",
        "    tweet = re.sub(r\"RondaRousey\", \"Ronda Rousey\", tweet)\n",
        "    tweet = re.sub(r\"MTVHottest\", \"MTV Hottest\", tweet)\n",
        "    tweet = re.sub(r\"TrapMusic\", \"Trap Music\", tweet)\n",
        "    tweet = re.sub(r\"ProphetMuhammad\", \"Prophet Muhammad\", tweet)\n",
        "    tweet = re.sub(r\"PantherAttack\", \"Panther Attack\", tweet)\n",
        "    tweet = re.sub(r\"StrategicPatience\", \"Strategic Patience\", tweet)\n",
        "    tweet = re.sub(r\"socialnews\", \"social news\", tweet)\n",
        "    tweet = re.sub(r\"NASAHurricane\", \"NASA Hurricane\", tweet)\n",
        "    tweet = re.sub(r\"onlinecommunities\", \"online communities\", tweet)\n",
        "    tweet = re.sub(r\"humanconsumption\", \"human consumption\", tweet)\n",
        "    tweet = re.sub(r\"Typhoon-Devastated\", \"Typhoon Devastated\", tweet)\n",
        "    tweet = re.sub(r\"Meat-Loving\", \"Meat Loving\", tweet)\n",
        "    tweet = re.sub(r\"facialabuse\", \"facial abuse\", tweet)\n",
        "    tweet = re.sub(r\"LakeCounty\", \"Lake County\", tweet)\n",
        "    tweet = re.sub(r\"BeingAuthor\", \"Being Author\", tweet)\n",
        "    tweet = re.sub(r\"withheavenly\", \"with heavenly\", tweet)\n",
        "    tweet = re.sub(r\"thankU\", \"thank you\", tweet)\n",
        "    tweet = re.sub(r\"iTunesMusic\", \"iTunes Music\", tweet)\n",
        "    tweet = re.sub(r\"OffensiveContent\", \"Offensive Content\", tweet)\n",
        "    tweet = re.sub(r\"WorstSummerJob\", \"Worst Summer Job\", tweet)\n",
        "    tweet = re.sub(r\"HarryBeCareful\", \"Harry Be Careful\", tweet)\n",
        "    tweet = re.sub(r\"NASASolarSystem\", \"NASA Solar System\", tweet)\n",
        "    tweet = re.sub(r\"animalrescue\", \"animal rescue\", tweet)\n",
        "    tweet = re.sub(r\"KurtSchlichter\", \"Kurt Schlichter\", tweet)\n",
        "    tweet = re.sub(r\"aRmageddon\", \"armageddon\", tweet)\n",
        "    tweet = re.sub(r\"Throwingknifes\", \"Throwing knives\", tweet)\n",
        "    tweet = re.sub(r\"GodsLove\", \"God's Love\", tweet)\n",
        "    tweet = re.sub(r\"bookboost\", \"book boost\", tweet)\n",
        "    tweet = re.sub(r\"ibooklove\", \"I book love\", tweet)\n",
        "    tweet = re.sub(r\"NestleIndia\", \"Nestle India\", tweet)\n",
        "    tweet = re.sub(r\"realDonaldTrump\", \"Donald Trump\", tweet)\n",
        "    tweet = re.sub(r\"DavidVonderhaar\", \"David Vonderhaar\", tweet)\n",
        "    tweet = re.sub(r\"CecilTheLion\", \"Cecil The Lion\", tweet)\n",
        "    tweet = re.sub(r\"weathernetwork\", \"weather network\", tweet)\n",
        "    tweet = re.sub(r\"withBioterrorism&use\", \"with Bioterrorism & use\", tweet)\n",
        "    tweet = re.sub(r\"Hostage&2\", \"Hostage & 2\", tweet)\n",
        "    tweet = re.sub(r\"GOPDebate\", \"GOP Debate\", tweet)\n",
        "    tweet = re.sub(r\"RickPerry\", \"Rick Perry\", tweet)\n",
        "    tweet = re.sub(r\"frontpage\", \"front page\", tweet)\n",
        "    tweet = re.sub(r\"NewsInTweets\", \"News In Tweets\", tweet)\n",
        "    tweet = re.sub(r\"ViralSpell\", \"Viral Spell\", tweet)\n",
        "    tweet = re.sub(r\"til_now\", \"until now\", tweet)\n",
        "    tweet = re.sub(r\"volcanoinRussia\", \"volcano in Russia\", tweet)\n",
        "    tweet = re.sub(r\"ZippedNews\", \"Zipped News\", tweet)\n",
        "    tweet = re.sub(r\"MicheleBachman\", \"Michele Bachman\", tweet)\n",
        "    tweet = re.sub(r\"53inch\", \"53 inch\", tweet)\n",
        "    tweet = re.sub(r\"KerrickTrial\", \"Kerrick Trial\", tweet)\n",
        "    tweet = re.sub(r\"abstorm\", \"Alberta Storm\", tweet)\n",
        "    tweet = re.sub(r\"Beyhive\", \"Beyonce hive\", tweet)\n",
        "    tweet = re.sub(r\"IDFire\", \"Idaho Fire\", tweet)\n",
        "    tweet = re.sub(r\"DETECTADO\", \"Detected\", tweet)\n",
        "    tweet = re.sub(r\"RockyFire\", \"Rocky Fire\", tweet)\n",
        "    tweet = re.sub(r\"Listen/Buy\", \"Listen / Buy\", tweet)\n",
        "    tweet = re.sub(r\"NickCannon\", \"Nick Cannon\", tweet)\n",
        "    tweet = re.sub(r\"FaroeIslands\", \"Faroe Islands\", tweet)\n",
        "    tweet = re.sub(r\"yycstorm\", \"Calgary Storm\", tweet)\n",
        "    tweet = re.sub(r\"IDPs:\", \"Internally Displaced People :\", tweet)\n",
        "    tweet = re.sub(r\"ArtistsUnited\", \"Artists United\", tweet)\n",
        "    tweet = re.sub(r\"ClaytonBryant\", \"Clayton Bryant\", tweet)\n",
        "    tweet = re.sub(r\"jimmyfallon\", \"jimmy fallon\", tweet)\n",
        "    tweet = re.sub(r\"justinbieber\", \"justin bieber\", tweet)\n",
        "    tweet = re.sub(r\"UTC2015\", \"UTC 2015\", tweet)\n",
        "    tweet = re.sub(r\"Time2015\", \"Time 2015\", tweet)\n",
        "    tweet = re.sub(r\"djicemoon\", \"dj icemoon\", tweet)\n",
        "    tweet = re.sub(r\"LivingSafely\", \"Living Safely\", tweet)\n",
        "    tweet = re.sub(r\"FIFA16\", \"Fifa 2016\", tweet)\n",
        "    tweet = re.sub(r\"thisiswhywecanthavenicethings\", \"this is why we cannot have nice things\", tweet)\n",
        "    tweet = re.sub(r\"bbcnews\", \"bbc news\", tweet)\n",
        "    tweet = re.sub(r\"UndergroundRailraod\", \"Underground Railraod\", tweet)\n",
        "    tweet = re.sub(r\"c4news\", \"c4 news\", tweet)\n",
        "    tweet = re.sub(r\"OBLITERATION\", \"obliteration\", tweet)\n",
        "    tweet = re.sub(r\"MUDSLIDE\", \"mudslide\", tweet)\n",
        "    tweet = re.sub(r\"NoSurrender\", \"No Surrender\", tweet)\n",
        "    tweet = re.sub(r\"NotExplained\", \"Not Explained\", tweet)\n",
        "    tweet = re.sub(r\"greatbritishbakeoff\", \"great british bake off\", tweet)\n",
        "    tweet = re.sub(r\"LondonFire\", \"London Fire\", tweet)\n",
        "    tweet = re.sub(r\"KOTAWeather\", \"KOTA Weather\", tweet)\n",
        "    tweet = re.sub(r\"LuchaUnderground\", \"Lucha Underground\", tweet)\n",
        "    tweet = re.sub(r\"KOIN6News\", \"KOIN 6 News\", tweet)\n",
        "    tweet = re.sub(r\"LiveOnK2\", \"Live On K2\", tweet)\n",
        "    tweet = re.sub(r\"9NewsGoldCoast\", \"9 News Gold Coast\", tweet)\n",
        "    tweet = re.sub(r\"nikeplus\", \"nike plus\", tweet)\n",
        "    tweet = re.sub(r\"david_cameron\", \"David Cameron\", tweet)\n",
        "    tweet = re.sub(r\"peterjukes\", \"Peter Jukes\", tweet)\n",
        "    tweet = re.sub(r\"JamesMelville\", \"James Melville\", tweet)\n",
        "    tweet = re.sub(r\"megynkelly\", \"Megyn Kelly\", tweet)\n",
        "    tweet = re.sub(r\"cnewslive\", \"C News Live\", tweet)\n",
        "    tweet = re.sub(r\"JamaicaObserver\", \"Jamaica Observer\", tweet)\n",
        "    tweet = re.sub(r\"TweetLikeItsSeptember11th2001\", \"Tweet like it is september 11th 2001\", tweet)\n",
        "    tweet = re.sub(r\"cbplawyers\", \"cbp lawyers\", tweet)\n",
        "    tweet = re.sub(r\"fewmoretweets\", \"few more tweets\", tweet)\n",
        "    tweet = re.sub(r\"BlackLivesMatter\", \"Black Lives Matter\", tweet)\n",
        "    tweet = re.sub(r\"cjoyner\", \"Chris Joyner\", tweet)\n",
        "    tweet = re.sub(r\"ENGvAUS\", \"England vs Australia\", tweet)\n",
        "    tweet = re.sub(r\"ScottWalker\", \"Scott Walker\", tweet)\n",
        "    tweet = re.sub(r\"MikeParrActor\", \"Michael Parr\", tweet)\n",
        "    tweet = re.sub(r\"4PlayThursdays\", \"Foreplay Thursdays\", tweet)\n",
        "    tweet = re.sub(r\"TGF2015\", \"Tontitown Grape Festival\", tweet)\n",
        "    tweet = re.sub(r\"realmandyrain\", \"Mandy Rain\", tweet)\n",
        "    tweet = re.sub(r\"GraysonDolan\", \"Grayson Dolan\", tweet)\n",
        "    tweet = re.sub(r\"ApolloBrown\", \"Apollo Brown\", tweet)\n",
        "    tweet = re.sub(r\"saddlebrooke\", \"Saddlebrooke\", tweet)\n",
        "    tweet = re.sub(r\"TontitownGrape\", \"Tontitown Grape\", tweet)\n",
        "    tweet = re.sub(r\"AbbsWinston\", \"Abbs Winston\", tweet)\n",
        "    tweet = re.sub(r\"ShaunKing\", \"Shaun King\", tweet)\n",
        "    tweet = re.sub(r\"MeekMill\", \"Meek Mill\", tweet)\n",
        "    tweet = re.sub(r\"TornadoGiveaway\", \"Tornado Giveaway\", tweet)\n",
        "    tweet = re.sub(r\"GRupdates\", \"GR updates\", tweet)\n",
        "    tweet = re.sub(r\"SouthDowns\", \"South Downs\", tweet)\n",
        "    tweet = re.sub(r\"braininjury\", \"brain injury\", tweet)\n",
        "    tweet = re.sub(r\"auspol\", \"Australian politics\", tweet)\n",
        "    tweet = re.sub(r\"PlannedParenthood\", \"Planned Parenthood\", tweet)\n",
        "    tweet = re.sub(r\"calgaryweather\", \"Calgary Weather\", tweet)\n",
        "    tweet = re.sub(r\"weallheartonedirection\", \"we all heart one direction\", tweet)\n",
        "    tweet = re.sub(r\"edsheeran\", \"Ed Sheeran\", tweet)\n",
        "    tweet = re.sub(r\"TrueHeroes\", \"True Heroes\", tweet)\n",
        "    tweet = re.sub(r\"S3XLEAK\", \"sex leak\", tweet)\n",
        "    tweet = re.sub(r\"ComplexMag\", \"Complex Magazine\", tweet)\n",
        "    tweet = re.sub(r\"TheAdvocateMag\", \"The Advocate Magazine\", tweet)\n",
        "    tweet = re.sub(r\"CityofCalgary\", \"City of Calgary\", tweet)\n",
        "    tweet = re.sub(r\"EbolaOutbreak\", \"Ebola Outbreak\", tweet)\n",
        "    tweet = re.sub(r\"SummerFate\", \"Summer Fate\", tweet)\n",
        "    tweet = re.sub(r\"RAmag\", \"Royal Academy Magazine\", tweet)\n",
        "    tweet = re.sub(r\"offers2go\", \"offers to go\", tweet)\n",
        "    tweet = re.sub(r\"foodscare\", \"food scare\", tweet)\n",
        "    tweet = re.sub(r\"MNPDNashville\", \"Metropolitan Nashville Police Department\", tweet)\n",
        "    tweet = re.sub(r\"TfLBusAlerts\", \"TfL Bus Alerts\", tweet)\n",
        "    tweet = re.sub(r\"GamerGate\", \"Gamer Gate\", tweet)\n",
        "    tweet = re.sub(r\"IHHen\", \"Humanitarian Relief\", tweet)\n",
        "    tweet = re.sub(r\"spinningbot\", \"spinning bot\", tweet)\n",
        "    tweet = re.sub(r\"ModiMinistry\", \"Modi Ministry\", tweet)\n",
        "    tweet = re.sub(r\"TAXIWAYS\", \"taxi ways\", tweet)\n",
        "    tweet = re.sub(r\"Calum5SOS\", \"Calum Hood\", tweet)\n",
        "    tweet = re.sub(r\"po_st\", \"po.st\", tweet)\n",
        "    tweet = re.sub(r\"scoopit\", \"scoop.it\", tweet)\n",
        "    tweet = re.sub(r\"UltimaLucha\", \"Ultima Lucha\", tweet)\n",
        "    tweet = re.sub(r\"JonathanFerrell\", \"Jonathan Ferrell\", tweet)\n",
        "    tweet = re.sub(r\"aria_ahrary\", \"Aria Ahrary\", tweet)\n",
        "    tweet = re.sub(r\"rapidcity\", \"Rapid City\", tweet)\n",
        "    tweet = re.sub(r\"OutBid\", \"outbid\", tweet)\n",
        "    tweet = re.sub(r\"lavenderpoetrycafe\", \"lavender poetry cafe\", tweet)\n",
        "    tweet = re.sub(r\"EudryLantiqua\", \"Eudry Lantiqua\", tweet)\n",
        "    tweet = re.sub(r\"15PM\", \"15 PM\", tweet)\n",
        "    tweet = re.sub(r\"OriginalFunko\", \"Funko\", tweet)\n",
        "    tweet = re.sub(r\"rightwaystan\", \"Richard Tan\", tweet)\n",
        "    tweet = re.sub(r\"CindyNoonan\", \"Cindy Noonan\", tweet)\n",
        "    tweet = re.sub(r\"RT_America\", \"RT America\", tweet)\n",
        "    tweet = re.sub(r\"narendramodi\", \"Narendra Modi\", tweet)\n",
        "    tweet = re.sub(r\"BakeOffFriends\", \"Bake Off Friends\", tweet)\n",
        "    tweet = re.sub(r\"TeamHendrick\", \"Hendrick Motorsports\", tweet)\n",
        "    tweet = re.sub(r\"alexbelloli\", \"Alex Belloli\", tweet)\n",
        "    tweet = re.sub(r\"itsjustinstuart\", \"Justin Stuart\", tweet)\n",
        "    tweet = re.sub(r\"gunsense\", \"gun sense\", tweet)\n",
        "    tweet = re.sub(r\"DebateQuestionsWeWantToHear\", \"debate questions we want to hear\", tweet)\n",
        "    tweet = re.sub(r\"RoyalCarribean\", \"Royal Carribean\", tweet)\n",
        "    tweet = re.sub(r\"samanthaturne19\", \"Samantha Turner\", tweet)\n",
        "    tweet = re.sub(r\"JonVoyage\", \"Jon Stewart\", tweet)\n",
        "    tweet = re.sub(r\"renew911health\", \"renew 911 health\", tweet)\n",
        "    tweet = re.sub(r\"SuryaRay\", \"Surya Ray\", tweet)\n",
        "    tweet = re.sub(r\"pattonoswalt\", \"Patton Oswalt\", tweet)\n",
        "    tweet = re.sub(r\"minhazmerchant\", \"Minhaz Merchant\", tweet)\n",
        "    tweet = re.sub(r\"TLVFaces\", \"Israel Diaspora Coalition\", tweet)\n",
        "    tweet = re.sub(r\"pmarca\", \"Marc Andreessen\", tweet)\n",
        "    tweet = re.sub(r\"pdx911\", \"Portland Police\", tweet)\n",
        "    tweet = re.sub(r\"jamaicaplain\", \"Jamaica Plain\", tweet)\n",
        "    tweet = re.sub(r\"Japton\", \"Arkansas\", tweet)\n",
        "    tweet = re.sub(r\"RouteComplex\", \"Route Complex\", tweet)\n",
        "    tweet = re.sub(r\"INSubcontinent\", \"Indian Subcontinent\", tweet)\n",
        "    tweet = re.sub(r\"NJTurnpike\", \"New Jersey Turnpike\", tweet)\n",
        "    tweet = re.sub(r\"Politifiact\", \"PolitiFact\", tweet)\n",
        "    tweet = re.sub(r\"Hiroshima70\", \"Hiroshima\", tweet)\n",
        "    tweet = re.sub(r\"GMMBC\", \"Greater Mt Moriah Baptist Church\", tweet)\n",
        "    tweet = re.sub(r\"versethe\", \"verse the\", tweet)\n",
        "    tweet = re.sub(r\"TubeStrike\", \"Tube Strike\", tweet)\n",
        "    tweet = re.sub(r\"MissionHills\", \"Mission Hills\", tweet)\n",
        "    tweet = re.sub(r\"ProtectDenaliWolves\", \"Protect Denali Wolves\", tweet)\n",
        "    tweet = re.sub(r\"NANKANA\", \"Nankana\", tweet)\n",
        "    tweet = re.sub(r\"SAHIB\", \"Sahib\", tweet)\n",
        "    tweet = re.sub(r\"PAKPATTAN\", \"Pakpattan\", tweet)\n",
        "    tweet = re.sub(r\"Newz_Sacramento\", \"News Sacramento\", tweet)\n",
        "    tweet = re.sub(r\"gofundme\", \"go fund me\", tweet)\n",
        "    tweet = re.sub(r\"pmharper\", \"Stephen Harper\", tweet)\n",
        "    tweet = re.sub(r\"IvanBerroa\", \"Ivan Berroa\", tweet)\n",
        "    tweet = re.sub(r\"LosDelSonido\", \"Los Del Sonido\", tweet)\n",
        "    tweet = re.sub(r\"bancodeseries\", \"banco de series\", tweet)\n",
        "    tweet = re.sub(r\"timkaine\", \"Tim Kaine\", tweet)\n",
        "    tweet = re.sub(r\"IdentityTheft\", \"Identity Theft\", tweet)\n",
        "    tweet = re.sub(r\"AllLivesMatter\", \"All Lives Matter\", tweet)\n",
        "    tweet = re.sub(r\"mishacollins\", \"Misha Collins\", tweet)\n",
        "    tweet = re.sub(r\"BillNeelyNBC\", \"Bill Neely\", tweet)\n",
        "    tweet = re.sub(r\"BeClearOnCancer\", \"be clear on cancer\", tweet)\n",
        "    tweet = re.sub(r\"Kowing\", \"Knowing\", tweet)\n",
        "    tweet = re.sub(r\"ScreamQueens\", \"Scream Queens\", tweet)\n",
        "    tweet = re.sub(r\"AskCharley\", \"Ask Charley\", tweet)\n",
        "    tweet = re.sub(r\"BlizzHeroes\", \"Heroes of the Storm\", tweet)\n",
        "    tweet = re.sub(r\"BradleyBrad47\", \"Bradley Brad\", tweet)\n",
        "    tweet = re.sub(r\"HannaPH\", \"Typhoon Hanna\", tweet)\n",
        "    tweet = re.sub(r\"meinlcymbals\", \"MEINL Cymbals\", tweet)\n",
        "    tweet = re.sub(r\"Ptbo\", \"Peterborough\", tweet)\n",
        "    tweet = re.sub(r\"cnnbrk\", \"CNN Breaking News\", tweet)\n",
        "    tweet = re.sub(r\"IndianNews\", \"Indian News\", tweet)\n",
        "    tweet = re.sub(r\"savebees\", \"save bees\", tweet)\n",
        "    tweet = re.sub(r\"GreenHarvard\", \"Green Harvard\", tweet)\n",
        "    tweet = re.sub(r\"StandwithPP\", \"Stand with planned parenthood\", tweet)\n",
        "    tweet = re.sub(r\"hermancranston\", \"Herman Cranston\", tweet)\n",
        "    tweet = re.sub(r\"WMUR9\", \"WMUR-TV\", tweet)\n",
        "    tweet = re.sub(r\"RockBottomRadFM\", \"Rock Bottom Radio\", tweet)\n",
        "    tweet = re.sub(r\"ameenshaikh3\", \"Ameen Shaikh\", tweet)\n",
        "    tweet = re.sub(r\"ProSyn\", \"Project Syndicate\", tweet)\n",
        "    tweet = re.sub(r\"Daesh\", \"ISIS\", tweet)\n",
        "    tweet = re.sub(r\"s2g\", \"swear to god\", tweet)\n",
        "    tweet = re.sub(r\"listenlive\", \"listen live\", tweet)\n",
        "    tweet = re.sub(r\"CDCgov\", \"Centers for Disease Control and Prevention\", tweet)\n",
        "    tweet = re.sub(r\"FoxNew\", \"Fox News\", tweet)\n",
        "    tweet = re.sub(r\"CBSBigBrother\", \"Big Brother\", tweet)\n",
        "    tweet = re.sub(r\"JulieDiCaro\", \"Julie DiCaro\", tweet)\n",
        "    tweet = re.sub(r\"theadvocatemag\", \"The Advocate Magazine\", tweet)\n",
        "    tweet = re.sub(r\"RohnertParkDPS\", \"Rohnert Park Police Department\", tweet)\n",
        "    tweet = re.sub(r\"THISIZBWRIGHT\", \"Bonnie Wright\", tweet)\n",
        "    tweet = re.sub(r\"Popularmmos\", \"Popular MMOs\", tweet)\n",
        "    tweet = re.sub(r\"WildHorses\", \"Wild Horses\", tweet)\n",
        "    tweet = re.sub(r\"FantasticFour\", \"Fantastic Four\", tweet)\n",
        "    tweet = re.sub(r\"HORNDALE\", \"Horndale\", tweet)\n",
        "    tweet = re.sub(r\"PINER\", \"Piner\", tweet)\n",
        "    tweet = re.sub(r\"BathAndNorthEastSomerset\", \"Bath and North East Somerset\", tweet)\n",
        "    tweet = re.sub(r\"thatswhatfriendsarefor\", \"that is what friends are for\", tweet)\n",
        "    tweet = re.sub(r\"residualincome\", \"residual income\", tweet)\n",
        "    tweet = re.sub(r\"YahooNewsDigest\", \"Yahoo News Digest\", tweet)\n",
        "    tweet = re.sub(r\"MalaysiaAirlines\", \"Malaysia Airlines\", tweet)\n",
        "    tweet = re.sub(r\"AmazonDeals\", \"Amazon Deals\", tweet)\n",
        "    tweet = re.sub(r\"MissCharleyWebb\", \"Charley Webb\", tweet)\n",
        "    tweet = re.sub(r\"shoalstraffic\", \"shoals traffic\", tweet)\n",
        "    tweet = re.sub(r\"GeorgeFoster72\", \"George Foster\", tweet)\n",
        "    tweet = re.sub(r\"pop2015\", \"pop 2015\", tweet)\n",
        "    tweet = re.sub(r\"_PokemonCards_\", \"Pokemon Cards\", tweet)\n",
        "    tweet = re.sub(r\"DianneG\", \"Dianne Gallagher\", tweet)\n",
        "    tweet = re.sub(r\"KashmirConflict\", \"Kashmir Conflict\", tweet)\n",
        "    tweet = re.sub(r\"BritishBakeOff\", \"British Bake Off\", tweet)\n",
        "    tweet = re.sub(r\"FreeKashmir\", \"Free Kashmir\", tweet)\n",
        "    tweet = re.sub(r\"mattmosley\", \"Matt Mosley\", tweet)\n",
        "    tweet = re.sub(r\"BishopFred\", \"Bishop Fred\", tweet)\n",
        "    tweet = re.sub(r\"EndConflict\", \"End Conflict\", tweet)\n",
        "    tweet = re.sub(r\"EndOccupation\", \"End Occupation\", tweet)\n",
        "    tweet = re.sub(r\"UNHEALED\", \"unhealed\", tweet)\n",
        "    tweet = re.sub(r\"CharlesDagnall\", \"Charles Dagnall\", tweet)\n",
        "    tweet = re.sub(r\"Latestnews\", \"Latest news\", tweet)\n",
        "    tweet = re.sub(r\"KindleCountdown\", \"Kindle Countdown\", tweet)\n",
        "    tweet = re.sub(r\"NoMoreHandouts\", \"No More Handouts\", tweet)\n",
        "    tweet = re.sub(r\"datingtips\", \"dating tips\", tweet)\n",
        "    tweet = re.sub(r\"charlesadler\", \"Charles Adler\", tweet)\n",
        "    tweet = re.sub(r\"twia\", \"Texas Windstorm Insurance Association\", tweet)\n",
        "    tweet = re.sub(r\"txlege\", \"Texas Legislature\", tweet)\n",
        "    tweet = re.sub(r\"WindstormInsurer\", \"Windstorm Insurer\", tweet)\n",
        "    tweet = re.sub(r\"Newss\", \"News\", tweet)\n",
        "    tweet = re.sub(r\"hempoil\", \"hemp oil\", tweet)\n",
        "    tweet = re.sub(r\"CommoditiesAre\", \"Commodities are\", tweet)\n",
        "    tweet = re.sub(r\"tubestrike\", \"tube strike\", tweet)\n",
        "    tweet = re.sub(r\"JoeNBC\", \"Joe Scarborough\", tweet)\n",
        "    tweet = re.sub(r\"LiteraryCakes\", \"Literary Cakes\", tweet)\n",
        "    tweet = re.sub(r\"TI5\", \"The International 5\", tweet)\n",
        "    tweet = re.sub(r\"thehill\", \"the hill\", tweet)\n",
        "    tweet = re.sub(r\"3others\", \"3 others\", tweet)\n",
        "    tweet = re.sub(r\"stighefootball\", \"Sam Tighe\", tweet)\n",
        "    tweet = re.sub(r\"whatstheimportantvideo\", \"what is the important video\", tweet)\n",
        "    tweet = re.sub(r\"ClaudioMeloni\", \"Claudio Meloni\", tweet)\n",
        "    tweet = re.sub(r\"DukeSkywalker\", \"Duke Skywalker\", tweet)\n",
        "    tweet = re.sub(r\"carsonmwr\", \"Fort Carson\", tweet)\n",
        "    tweet = re.sub(r\"offdishduty\", \"off dish duty\", tweet)\n",
        "    tweet = re.sub(r\"andword\", \"and word\", tweet)\n",
        "    tweet = re.sub(r\"rhodeisland\", \"Rhode Island\", tweet)\n",
        "    tweet = re.sub(r\"easternoregon\", \"Eastern Oregon\", tweet)\n",
        "    tweet = re.sub(r\"WAwildfire\", \"Washington Wildfire\", tweet)\n",
        "    tweet = re.sub(r\"fingerrockfire\", \"Finger Rock Fire\", tweet)\n",
        "    tweet = re.sub(r\"57am\", \"57 am\", tweet)\n",
        "    tweet = re.sub(r\"fingerrockfire\", \"Finger Rock Fire\", tweet)\n",
        "    tweet = re.sub(r\"JacobHoggard\", \"Jacob Hoggard\", tweet)\n",
        "    tweet = re.sub(r\"newnewnew\", \"new new new\", tweet)\n",
        "    tweet = re.sub(r\"under50\", \"under 50\", tweet)\n",
        "    tweet = re.sub(r\"getitbeforeitsgone\", \"get it before it is gone\", tweet)\n",
        "    tweet = re.sub(r\"freshoutofthebox\", \"fresh out of the box\", tweet)\n",
        "    tweet = re.sub(r\"amwriting\", \"am writing\", tweet)\n",
        "    tweet = re.sub(r\"Bokoharm\", \"Boko Haram\", tweet)\n",
        "    tweet = re.sub(r\"Nowlike\", \"Now like\", tweet)\n",
        "    tweet = re.sub(r\"seasonfrom\", \"season from\", tweet)\n",
        "    tweet = re.sub(r\"epicente\", \"epicenter\", tweet)\n",
        "    tweet = re.sub(r\"epicenterr\", \"epicenter\", tweet)\n",
        "    tweet = re.sub(r\"sicklife\", \"sick life\", tweet)\n",
        "    tweet = re.sub(r\"yycweather\", \"Calgary Weather\", tweet)\n",
        "    tweet = re.sub(r\"calgarysun\", \"Calgary Sun\", tweet)\n",
        "    tweet = re.sub(r\"approachng\", \"approaching\", tweet)\n",
        "    tweet = re.sub(r\"evng\", \"evening\", tweet)\n",
        "    tweet = re.sub(r\"Sumthng\", \"something\", tweet)\n",
        "    tweet = re.sub(r\"EllenPompeo\", \"Ellen Pompeo\", tweet)\n",
        "    tweet = re.sub(r\"shondarhimes\", \"Shonda Rhimes\", tweet)\n",
        "    tweet = re.sub(r\"ABCNetwork\", \"ABC Network\", tweet)\n",
        "    tweet = re.sub(r\"SushmaSwaraj\", \"Sushma Swaraj\", tweet)\n",
        "    tweet = re.sub(r\"pray4japan\", \"Pray for Japan\", tweet)\n",
        "    tweet = re.sub(r\"hope4japan\", \"Hope for Japan\", tweet)\n",
        "    tweet = re.sub(r\"Illusionimagess\", \"Illusion images\", tweet)\n",
        "    tweet = re.sub(r\"SummerUnderTheStars\", \"Summer Under The Stars\", tweet)\n",
        "    tweet = re.sub(r\"ShallWeDance\", \"Shall We Dance\", tweet)\n",
        "    tweet = re.sub(r\"TCMParty\", \"TCM Party\", tweet)\n",
        "    tweet = re.sub(r\"marijuananews\", \"marijuana news\", tweet)\n",
        "    tweet = re.sub(r\"onbeingwithKristaTippett\", \"on being with Krista Tippett\", tweet)\n",
        "    tweet = re.sub(r\"Beingtweets\", \"Being tweets\", tweet)\n",
        "    tweet = re.sub(r\"newauthors\", \"new authors\", tweet)\n",
        "    tweet = re.sub(r\"remedyyyy\", \"remedy\", tweet)\n",
        "    tweet = re.sub(r\"44PM\", \"44 PM\", tweet)\n",
        "    tweet = re.sub(r\"HeadlinesApp\", \"Headlines App\", tweet)\n",
        "    tweet = re.sub(r\"40PM\", \"40 PM\", tweet)\n",
        "    tweet = re.sub(r\"myswc\", \"Severe Weather Center\", tweet)\n",
        "    tweet = re.sub(r\"ithats\", \"that is\", tweet)\n",
        "    tweet = re.sub(r\"icouldsitinthismomentforever\", \"I could sit in this moment forever\", tweet)\n",
        "    tweet = re.sub(r\"FatLoss\", \"Fat Loss\", tweet)\n",
        "    tweet = re.sub(r\"02PM\", \"02 PM\", tweet)\n",
        "    tweet = re.sub(r\"MetroFmTalk\", \"Metro Fm Talk\", tweet)\n",
        "    tweet = re.sub(r\"Bstrd\", \"bastard\", tweet)\n",
        "    tweet = re.sub(r\"bldy\", \"bloody\", tweet)\n",
        "    tweet = re.sub(r\"MetrofmTalk\", \"Metro Fm Talk\", tweet)\n",
        "    tweet = re.sub(r\"terrorismturn\", \"terrorism turn\", tweet)\n",
        "    tweet = re.sub(r\"BBCNewsAsia\", \"BBC News Asia\", tweet)\n",
        "    tweet = re.sub(r\"BehindTheScenes\", \"Behind The Scenes\", tweet)\n",
        "    tweet = re.sub(r\"GeorgeTakei\", \"George Takei\", tweet)\n",
        "    tweet = re.sub(r\"WomensWeeklyMag\", \"Womens Weekly Magazine\", tweet)\n",
        "    tweet = re.sub(r\"SurvivorsGuidetoEarth\", \"Survivors Guide to Earth\", tweet)\n",
        "    tweet = re.sub(r\"incubusband\", \"incubus band\", tweet)\n",
        "    tweet = re.sub(r\"Babypicturethis\", \"Baby picture this\", tweet)\n",
        "    tweet = re.sub(r\"BombEffects\", \"Bomb Effects\", tweet)\n",
        "    tweet = re.sub(r\"win10\", \"Windows 10\", tweet)\n",
        "    tweet = re.sub(r\"idkidk\", \"I do not know I do not know\", tweet)\n",
        "    tweet = re.sub(r\"TheWalkingDead\", \"The Walking Dead\", tweet)\n",
        "    tweet = re.sub(r\"amyschumer\", \"Amy Schumer\", tweet)\n",
        "    tweet = re.sub(r\"crewlist\", \"crew list\", tweet)\n",
        "    tweet = re.sub(r\"Erdogans\", \"Erdogan\", tweet)\n",
        "    tweet = re.sub(r\"BBCLive\", \"BBC Live\", tweet)\n",
        "    tweet = re.sub(r\"TonyAbbottMHR\", \"Tony Abbott\", tweet)\n",
        "    tweet = re.sub(r\"paulmyerscough\", \"Paul Myerscough\", tweet)\n",
        "    tweet = re.sub(r\"georgegallagher\", \"George Gallagher\", tweet)\n",
        "    tweet = re.sub(r\"JimmieJohnson\", \"Jimmie Johnson\", tweet)\n",
        "    tweet = re.sub(r\"pctool\", \"pc tool\", tweet)\n",
        "    tweet = re.sub(r\"DoingHashtagsRight\", \"Doing Hashtags Right\", tweet)\n",
        "    tweet = re.sub(r\"ThrowbackThursday\", \"Throwback Thursday\", tweet)\n",
        "    tweet = re.sub(r\"SnowBackSunday\", \"Snowback Sunday\", tweet)\n",
        "    tweet = re.sub(r\"LakeEffect\", \"Lake Effect\", tweet)\n",
        "    tweet = re.sub(r\"RTphotographyUK\", \"Richard Thomas Photography UK\", tweet)\n",
        "    tweet = re.sub(r\"BigBang_CBS\", \"Big Bang CBS\", tweet)\n",
        "    tweet = re.sub(r\"writerslife\", \"writers life\", tweet)\n",
        "    tweet = re.sub(r\"NaturalBirth\", \"Natural Birth\", tweet)\n",
        "    tweet = re.sub(r\"UnusualWords\", \"Unusual Words\", tweet)\n",
        "    tweet = re.sub(r\"wizkhalifa\", \"Wiz Khalifa\", tweet)\n",
        "    tweet = re.sub(r\"acreativedc\", \"a creative DC\", tweet)\n",
        "    tweet = re.sub(r\"vscodc\", \"vsco DC\", tweet)\n",
        "    tweet = re.sub(r\"VSCOcam\", \"vsco camera\", tweet)\n",
        "    tweet = re.sub(r\"TheBEACHDC\", \"The beach DC\", tweet)\n",
        "    tweet = re.sub(r\"buildingmuseum\", \"building museum\", tweet)\n",
        "    tweet = re.sub(r\"WorldOil\", \"World Oil\", tweet)\n",
        "    tweet = re.sub(r\"redwedding\", \"red wedding\", tweet)\n",
        "    tweet = re.sub(r\"AmazingRaceCanada\", \"Amazing Race Canada\", tweet)\n",
        "    tweet = re.sub(r\"WakeUpAmerica\", \"Wake Up America\", tweet)\n",
        "    tweet = re.sub(r\"\\\\Allahuakbar\\\\\", \"Allahu Akbar\", tweet)\n",
        "    tweet = re.sub(r\"bleased\", \"blessed\", tweet)\n",
        "    tweet = re.sub(r\"nigeriantribune\", \"Nigerian Tribune\", tweet)\n",
        "    tweet = re.sub(r\"HIDEO_KOJIMA_EN\", \"Hideo Kojima\", tweet)\n",
        "    tweet = re.sub(r\"FusionFestival\", \"Fusion Festival\", tweet)\n",
        "    tweet = re.sub(r\"50Mixed\", \"50 Mixed\", tweet)\n",
        "    tweet = re.sub(r\"NoAgenda\", \"No Agenda\", tweet)\n",
        "    tweet = re.sub(r\"WhiteGenocide\", \"White Genocide\", tweet)\n",
        "    tweet = re.sub(r\"dirtylying\", \"dirty lying\", tweet)\n",
        "    tweet = re.sub(r\"SyrianRefugees\", \"Syrian Refugees\", tweet)\n",
        "    tweet = re.sub(r\"changetheworld\", \"change the world\", tweet)\n",
        "    tweet = re.sub(r\"Ebolacase\", \"Ebola case\", tweet)\n",
        "    tweet = re.sub(r\"mcgtech\", \"mcg technologies\", tweet)\n",
        "    tweet = re.sub(r\"withweapons\", \"with weapons\", tweet)\n",
        "    tweet = re.sub(r\"advancedwarfare\", \"advanced warfare\", tweet)\n",
        "    tweet = re.sub(r\"letsFootball\", \"let us Football\", tweet)\n",
        "    tweet = re.sub(r\"LateNiteMix\", \"late night mix\", tweet)\n",
        "    tweet = re.sub(r\"PhilCollinsFeed\", \"Phil Collins\", tweet)\n",
        "    tweet = re.sub(r\"RudyHavenstein\", \"Rudy Havenstein\", tweet)\n",
        "    tweet = re.sub(r\"22PM\", \"22 PM\", tweet)\n",
        "    tweet = re.sub(r\"54am\", \"54 AM\", tweet)\n",
        "    tweet = re.sub(r\"38am\", \"38 AM\", tweet)\n",
        "    tweet = re.sub(r\"OldFolkExplainStuff\", \"Old Folk Explain Stuff\", tweet)\n",
        "    tweet = re.sub(r\"BlacklivesMatter\", \"Black Lives Matter\", tweet)\n",
        "    tweet = re.sub(r\"InsaneLimits\", \"Insane Limits\", tweet)\n",
        "    tweet = re.sub(r\"youcantsitwithus\", \"you cannot sit with us\", tweet)\n",
        "    tweet = re.sub(r\"2k15\", \"2015\", tweet)\n",
        "    tweet = re.sub(r\"TheIran\", \"Iran\", tweet)\n",
        "    tweet = re.sub(r\"JimmyFallon\", \"Jimmy Fallon\", tweet)\n",
        "    tweet = re.sub(r\"AlbertBrooks\", \"Albert Brooks\", tweet)\n",
        "    tweet = re.sub(r\"defense_news\", \"defense news\", tweet)\n",
        "    tweet = re.sub(r\"nuclearrcSA\", \"Nuclear Risk Control Self Assessment\", tweet)\n",
        "    tweet = re.sub(r\"Auspol\", \"Australia Politics\", tweet)\n",
        "    tweet = re.sub(r\"NuclearPower\", \"Nuclear Power\", tweet)\n",
        "    tweet = re.sub(r\"WhiteTerrorism\", \"White Terrorism\", tweet)\n",
        "    tweet = re.sub(r\"truthfrequencyradio\", \"Truth Frequency Radio\", tweet)\n",
        "    tweet = re.sub(r\"ErasureIsNotEquality\", \"Erasure is not equality\", tweet)\n",
        "    tweet = re.sub(r\"ProBonoNews\", \"Pro Bono News\", tweet)\n",
        "    tweet = re.sub(r\"JakartaPost\", \"Jakarta Post\", tweet)\n",
        "    tweet = re.sub(r\"toopainful\", \"too painful\", tweet)\n",
        "    tweet = re.sub(r\"melindahaunton\", \"Melinda Haunton\", tweet)\n",
        "    tweet = re.sub(r\"NoNukes\", \"No Nukes\", tweet)\n",
        "    tweet = re.sub(r\"curryspcworld\", \"Currys PC World\", tweet)\n",
        "    tweet = re.sub(r\"ineedcake\", \"I need cake\", tweet)\n",
        "    tweet = re.sub(r\"blackforestgateau\", \"black forest gateau\", tweet)\n",
        "    tweet = re.sub(r\"BBCOne\", \"BBC One\", tweet)\n",
        "    tweet = re.sub(r\"AlexxPage\", \"Alex Page\", tweet)\n",
        "    tweet = re.sub(r\"jonathanserrie\", \"Jonathan Serrie\", tweet)\n",
        "    tweet = re.sub(r\"SocialJerkBlog\", \"Social Jerk Blog\", tweet)\n",
        "    tweet = re.sub(r\"ChelseaVPeretti\", \"Chelsea Peretti\", tweet)\n",
        "    tweet = re.sub(r\"irongiant\", \"iron giant\", tweet)\n",
        "    tweet = re.sub(r\"RonFunches\", \"Ron Funches\", tweet)\n",
        "    tweet = re.sub(r\"TimCook\", \"Tim Cook\", tweet)\n",
        "    tweet = re.sub(r\"sebastianstanisaliveandwell\", \"Sebastian Stan is alive and well\", tweet)\n",
        "    tweet = re.sub(r\"Madsummer\", \"Mad summer\", tweet)\n",
        "    tweet = re.sub(r\"NowYouKnow\", \"Now you know\", tweet)\n",
        "    tweet = re.sub(r\"concertphotography\", \"concert photography\", tweet)\n",
        "    tweet = re.sub(r\"TomLandry\", \"Tom Landry\", tweet)\n",
        "    tweet = re.sub(r\"showgirldayoff\", \"show girl day off\", tweet)\n",
        "    tweet = re.sub(r\"Yougslavia\", \"Yugoslavia\", tweet)\n",
        "    tweet = re.sub(r\"QuantumDataInformatics\", \"Quantum Data Informatics\", tweet)\n",
        "    tweet = re.sub(r\"FromTheDesk\", \"From The Desk\", tweet)\n",
        "    tweet = re.sub(r\"TheaterTrial\", \"Theater Trial\", tweet)\n",
        "    tweet = re.sub(r\"CatoInstitute\", \"Cato Institute\", tweet)\n",
        "    tweet = re.sub(r\"EmekaGift\", \"Emeka Gift\", tweet)\n",
        "    tweet = re.sub(r\"LetsBe_Rational\", \"Let us be rational\", tweet)\n",
        "    tweet = re.sub(r\"Cynicalreality\", \"Cynical reality\", tweet)\n",
        "    tweet = re.sub(r\"FredOlsenCruise\", \"Fred Olsen Cruise\", tweet)\n",
        "    tweet = re.sub(r\"NotSorry\", \"not sorry\", tweet)\n",
        "    tweet = re.sub(r\"UseYourWords\", \"use your words\", tweet)\n",
        "    tweet = re.sub(r\"WordoftheDay\", \"word of the day\", tweet)\n",
        "    tweet = re.sub(r\"Dictionarycom\", \"Dictionary.com\", tweet)\n",
        "    tweet = re.sub(r\"TheBrooklynLife\", \"The Brooklyn Life\", tweet)\n",
        "    tweet = re.sub(r\"jokethey\", \"joke they\", tweet)\n",
        "    tweet = re.sub(r\"nflweek1picks\", \"NFL week 1 picks\", tweet)\n",
        "    tweet = re.sub(r\"uiseful\", \"useful\", tweet)\n",
        "    tweet = re.sub(r\"JusticeDotOrg\", \"The American Association for Justice\", tweet)\n",
        "    tweet = re.sub(r\"autoaccidents\", \"auto accidents\", tweet)\n",
        "    tweet = re.sub(r\"SteveGursten\", \"Steve Gursten\", tweet)\n",
        "    tweet = re.sub(r\"MichiganAutoLaw\", \"Michigan Auto Law\", tweet)\n",
        "    tweet = re.sub(r\"birdgang\", \"bird gang\", tweet)\n",
        "    tweet = re.sub(r\"nflnetwork\", \"NFL Network\", tweet)\n",
        "    tweet = re.sub(r\"NYDNSports\", \"NY Daily News Sports\", tweet)\n",
        "    tweet = re.sub(r\"RVacchianoNYDN\", \"Ralph Vacchiano NY Daily News\", tweet)\n",
        "    tweet = re.sub(r\"EdmontonEsks\", \"Edmonton Eskimos\", tweet)\n",
        "    tweet = re.sub(r\"david_brelsford\", \"David Brelsford\", tweet)\n",
        "    tweet = re.sub(r\"TOI_India\", \"The Times of India\", tweet)\n",
        "    tweet = re.sub(r\"hegot\", \"he got\", tweet)\n",
        "    tweet = re.sub(r\"SkinsOn9\", \"Skins on 9\", tweet)\n",
        "    tweet = re.sub(r\"sothathappened\", \"so that happened\", tweet)\n",
        "    tweet = re.sub(r\"LCOutOfDoors\", \"LC Out Of Doors\", tweet)\n",
        "    tweet = re.sub(r\"NationFirst\", \"Nation First\", tweet)\n",
        "    tweet = re.sub(r\"IndiaToday\", \"India Today\", tweet)\n",
        "    tweet = re.sub(r\"HLPS\", \"helps\", tweet)\n",
        "    tweet = re.sub(r\"HOSTAGESTHROSW\", \"hostages throw\", tweet)\n",
        "    tweet = re.sub(r\"SNCTIONS\", \"sanctions\", tweet)\n",
        "    tweet = re.sub(r\"BidTime\", \"Bid Time\", tweet)\n",
        "    tweet = re.sub(r\"crunchysensible\", \"crunchy sensible\", tweet)\n",
        "    tweet = re.sub(r\"RandomActsOfRomance\", \"Random acts of romance\", tweet)\n",
        "    tweet = re.sub(r\"MomentsAtHill\", \"Moments at hill\", tweet)\n",
        "    tweet = re.sub(r\"eatshit\", \"eat shit\", tweet)\n",
        "    tweet = re.sub(r\"liveleakfun\", \"live leak fun\", tweet)\n",
        "    tweet = re.sub(r\"SahelNews\", \"Sahel News\", tweet)\n",
        "    tweet = re.sub(r\"abc7newsbayarea\", \"ABC 7 News Bay Area\", tweet)\n",
        "    tweet = re.sub(r\"facilitiesmanagement\", \"facilities management\", tweet)\n",
        "    tweet = re.sub(r\"facilitydude\", \"facility dude\", tweet)\n",
        "    tweet = re.sub(r\"CampLogistics\", \"Camp logistics\", tweet)\n",
        "    tweet = re.sub(r\"alaskapublic\", \"Alaska public\", tweet)\n",
        "    tweet = re.sub(r\"MarketResearch\", \"Market Research\", tweet)\n",
        "    tweet = re.sub(r\"AccuracyEsports\", \"Accuracy Esports\", tweet)\n",
        "    tweet = re.sub(r\"TheBodyShopAust\", \"The Body Shop Australia\", tweet)\n",
        "    tweet = re.sub(r\"yychail\", \"Calgary hail\", tweet)\n",
        "    tweet = re.sub(r\"yyctraffic\", \"Calgary traffic\", tweet)\n",
        "    tweet = re.sub(r\"eliotschool\", \"eliot school\", tweet)\n",
        "    tweet = re.sub(r\"TheBrokenCity\", \"The Broken City\", tweet)\n",
        "    tweet = re.sub(r\"OldsFireDept\", \"Olds Fire Department\", tweet)\n",
        "    tweet = re.sub(r\"RiverComplex\", \"River Complex\", tweet)\n",
        "    tweet = re.sub(r\"fieldworksmells\", \"field work smells\", tweet)\n",
        "    tweet = re.sub(r\"IranElection\", \"Iran Election\", tweet)\n",
        "    tweet = re.sub(r\"glowng\", \"glowing\", tweet)\n",
        "    tweet = re.sub(r\"kindlng\", \"kindling\", tweet)\n",
        "    tweet = re.sub(r\"riggd\", \"rigged\", tweet)\n",
        "    tweet = re.sub(r\"slownewsday\", \"slow news day\", tweet)\n",
        "    tweet = re.sub(r\"MyanmarFlood\", \"Myanmar Flood\", tweet)\n",
        "    tweet = re.sub(r\"abc7chicago\", \"ABC 7 Chicago\", tweet)\n",
        "    tweet = re.sub(r\"copolitics\", \"Colorado Politics\", tweet)\n",
        "    tweet = re.sub(r\"AdilGhumro\", \"Adil Ghumro\", tweet)\n",
        "    tweet = re.sub(r\"netbots\", \"net bots\", tweet)\n",
        "    tweet = re.sub(r\"byebyeroad\", \"bye bye road\", tweet)\n",
        "    tweet = re.sub(r\"massiveflooding\", \"massive flooding\", tweet)\n",
        "    tweet = re.sub(r\"EndofUS\", \"End of United States\", tweet)\n",
        "    tweet = re.sub(r\"35PM\", \"35 PM\", tweet)\n",
        "    tweet = re.sub(r\"greektheatrela\", \"Greek Theatre Los Angeles\", tweet)\n",
        "    tweet = re.sub(r\"76mins\", \"76 minutes\", tweet)\n",
        "    tweet = re.sub(r\"publicsafetyfirst\", \"public safety first\", tweet)\n",
        "    tweet = re.sub(r\"livesmatter\", \"lives matter\", tweet)\n",
        "    tweet = re.sub(r\"myhometown\", \"my hometown\", tweet)\n",
        "    tweet = re.sub(r\"tankerfire\", \"tanker fire\", tweet)\n",
        "    tweet = re.sub(r\"MEMORIALDAY\", \"memorial day\", tweet)\n",
        "    tweet = re.sub(r\"MEMORIAL_DAY\", \"memorial day\", tweet)\n",
        "    tweet = re.sub(r\"instaxbooty\", \"instagram booty\", tweet)\n",
        "    tweet = re.sub(r\"Jerusalem_Post\", \"Jerusalem Post\", tweet)\n",
        "    tweet = re.sub(r\"WayneRooney_INA\", \"Wayne Rooney\", tweet)\n",
        "    tweet = re.sub(r\"VirtualReality\", \"Virtual Reality\", tweet)\n",
        "    tweet = re.sub(r\"OculusRift\", \"Oculus Rift\", tweet)\n",
        "    tweet = re.sub(r\"OwenJones84\", \"Owen Jones\", tweet)\n",
        "    tweet = re.sub(r\"jeremycorbyn\", \"Jeremy Corbyn\", tweet)\n",
        "    tweet = re.sub(r\"paulrogers002\", \"Paul Rogers\", tweet)\n",
        "    tweet = re.sub(r\"mortalkombatx\", \"Mortal Kombat X\", tweet)\n",
        "    tweet = re.sub(r\"mortalkombat\", \"Mortal Kombat\", tweet)\n",
        "    tweet = re.sub(r\"FilipeCoelho92\", \"Filipe Coelho\", tweet)\n",
        "    tweet = re.sub(r\"OnlyQuakeNews\", \"Only Quake News\", tweet)\n",
        "    tweet = re.sub(r\"kostumes\", \"costumes\", tweet)\n",
        "    tweet = re.sub(r\"YEEESSSS\", \"yes\", tweet)\n",
        "    tweet = re.sub(r\"ToshikazuKatayama\", \"Toshikazu Katayama\", tweet)\n",
        "    tweet = re.sub(r\"IntlDevelopment\", \"Intl Development\", tweet)\n",
        "    tweet = re.sub(r\"ExtremeWeather\", \"Extreme Weather\", tweet)\n",
        "    tweet = re.sub(r\"WereNotGruberVoters\", \"We are not gruber voters\", tweet)\n",
        "    tweet = re.sub(r\"NewsThousands\", \"News Thousands\", tweet)\n",
        "    tweet = re.sub(r\"EdmundAdamus\", \"Edmund Adamus\", tweet)\n",
        "    tweet = re.sub(r\"EyewitnessWV\", \"Eye witness WV\", tweet)\n",
        "    tweet = re.sub(r\"PhiladelphiaMuseu\", \"Philadelphia Museum\", tweet)\n",
        "    tweet = re.sub(r\"DublinComicCon\", \"Dublin Comic Con\", tweet)\n",
        "    tweet = re.sub(r\"NicholasBrendon\", \"Nicholas Brendon\", tweet)\n",
        "    tweet = re.sub(r\"Alltheway80s\", \"All the way 80s\", tweet)\n",
        "    tweet = re.sub(r\"FromTheField\", \"From the field\", tweet)\n",
        "    tweet = re.sub(r\"NorthIowa\", \"North Iowa\", tweet)\n",
        "    tweet = re.sub(r\"WillowFire\", \"Willow Fire\", tweet)\n",
        "    tweet = re.sub(r\"MadRiverComplex\", \"Mad River Complex\", tweet)\n",
        "    tweet = re.sub(r\"feelingmanly\", \"feeling manly\", tweet)\n",
        "    tweet = re.sub(r\"stillnotoverit\", \"still not over it\", tweet)\n",
        "    tweet = re.sub(r\"FortitudeValley\", \"Fortitude Valley\", tweet)\n",
        "    tweet = re.sub(r\"CoastpowerlineTramTr\", \"Coast powerline\", tweet)\n",
        "    tweet = re.sub(r\"ServicesGold\", \"Services Gold\", tweet)\n",
        "    tweet = re.sub(r\"NewsbrokenEmergency\", \"News broken emergency\", tweet)\n",
        "    tweet = re.sub(r\"Evaucation\", \"evacuation\", tweet)\n",
        "    tweet = re.sub(r\"leaveevacuateexitbe\", \"leave evacuate exit be\", tweet)\n",
        "    tweet = re.sub(r\"P_EOPLE\", \"PEOPLE\", tweet)\n",
        "    tweet = re.sub(r\"Tubestrike\", \"tube strike\", tweet)\n",
        "    tweet = re.sub(r\"CLASS_SICK\", \"CLASS SICK\", tweet)\n",
        "    tweet = re.sub(r\"localplumber\", \"local plumber\", tweet)\n",
        "    tweet = re.sub(r\"awesomejobsiri\", \"awesome job siri\", tweet)\n",
        "    tweet = re.sub(r\"PayForItHow\", \"Pay for it how\", tweet)\n",
        "    tweet = re.sub(r\"ThisIsAfrica\", \"This is Africa\", tweet)\n",
        "    tweet = re.sub(r\"crimeairnetwork\", \"crime air network\", tweet)\n",
        "    tweet = re.sub(r\"KimAcheson\", \"Kim Acheson\", tweet)\n",
        "    tweet = re.sub(r\"cityofcalgary\", \"City of Calgary\", tweet)\n",
        "    tweet = re.sub(r\"prosyndicate\", \"pro syndicate\", tweet)\n",
        "    tweet = re.sub(r\"660NEWS\", \"660 NEWS\", tweet)\n",
        "    tweet = re.sub(r\"BusInsMagazine\", \"Business Insurance Magazine\", tweet)\n",
        "    tweet = re.sub(r\"wfocus\", \"focus\", tweet)\n",
        "    tweet = re.sub(r\"ShastaDam\", \"Shasta Dam\", tweet)\n",
        "    tweet = re.sub(r\"go2MarkFranco\", \"Mark Franco\", tweet)\n",
        "    tweet = re.sub(r\"StephGHinojosa\", \"Steph Hinojosa\", tweet)\n",
        "    tweet = re.sub(r\"Nashgrier\", \"Nash Grier\", tweet)\n",
        "    tweet = re.sub(r\"NashNewVideo\", \"Nash new video\", tweet)\n",
        "    tweet = re.sub(r\"IWouldntGetElectedBecause\", \"I would not get elected because\", tweet)\n",
        "    tweet = re.sub(r\"SHGames\", \"Sledgehammer Games\", tweet)\n",
        "    tweet = re.sub(r\"bedhair\", \"bed hair\", tweet)\n",
        "    tweet = re.sub(r\"JoelHeyman\", \"Joel Heyman\", tweet)\n",
        "    tweet = re.sub(r\"viaYouTube\", \"via YouTube\", tweet)\n",
        "\n",
        "    # Urls\n",
        "    tweet = re.sub(r\"https?:\\/\\/t.co\\/[A-Za-z0-9]+\", \"\", tweet)\n",
        "\n",
        "    # Words with punctuations and special characters\n",
        "    punctuations = '@#!?+&*[]-%.:/();$=><|{}^' + \"'`\"\n",
        "    for p in punctuations:\n",
        "        tweet = tweet.replace(p, f' {p} ')\n",
        "\n",
        "    # ... and ..\n",
        "    tweet = tweet.replace('...', ' ... ')\n",
        "    if '...' not in tweet:\n",
        "        tweet = tweet.replace('..', ' ... ')\n",
        "\n",
        "    # Acronyms\n",
        "    tweet = re.sub(r\"MH370\", \"Malaysia Airlines Flight 370\", tweet)\n",
        "    tweet = re.sub(r\"mÌ¼sica\", \"music\", tweet)\n",
        "    tweet = re.sub(r\"okwx\", \"Oklahoma City Weather\", tweet)\n",
        "    tweet = re.sub(r\"arwx\", \"Arkansas Weather\", tweet)\n",
        "    tweet = re.sub(r\"gawx\", \"Georgia Weather\", tweet)\n",
        "    tweet = re.sub(r\"scwx\", \"South Carolina Weather\", tweet)\n",
        "    tweet = re.sub(r\"cawx\", \"California Weather\", tweet)\n",
        "    tweet = re.sub(r\"tnwx\", \"Tennessee Weather\", tweet)\n",
        "    tweet = re.sub(r\"azwx\", \"Arizona Weather\", tweet)\n",
        "    tweet = re.sub(r\"alwx\", \"Alabama Weather\", tweet)\n",
        "    tweet = re.sub(r\"wordpressdotcom\", \"wordpress\", tweet)\n",
        "    tweet = re.sub(r\"usNWSgov\", \"United States National Weather Service\", tweet)\n",
        "    tweet = re.sub(r\"Suruc\", \"Sanliurfa\", tweet)\n",
        "\n",
        "    # Grouping same words without embeddings\n",
        "    tweet = re.sub(r\"Bestnaijamade\", \"bestnaijamade\", tweet)\n",
        "    tweet = re.sub(r\"SOUDELOR\", \"Soudelor\", tweet)\n",
        "\n",
        "    return tweet\n"
      ],
      "metadata": {
        "id": "6_PLe9GwfY5F",
        "outputId": "55a665be-761a-48c8-f582-29ab47efaf45",
        "execution": {
          "iopub.status.busy": "2024-04-06T02:48:23.191006Z",
          "iopub.execute_input": "2024-04-06T02:48:23.191735Z",
          "iopub.status.idle": "2024-04-06T02:48:23.755241Z",
          "shell.execute_reply.started": "2024-04-06T02:48:23.191676Z",
          "shell.execute_reply": "2024-04-06T02:48:23.754194Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "CPU times: user 8 µs, sys: 0 ns, total: 8 µs\nWall time: 14.5 µs\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train['text'] = train['text'].apply(lambda s : clean(s))\n",
        "test['text'] = test['text'].apply(lambda s : clean(s))"
      ],
      "metadata": {
        "id": "L8K0CrWGkBtE",
        "execution": {
          "iopub.status.busy": "2024-04-06T02:48:23.75681Z",
          "iopub.execute_input": "2024-04-06T02:48:23.757167Z",
          "iopub.status.idle": "2024-04-06T02:57:03.750923Z",
          "shell.execute_reply.started": "2024-04-06T02:48:23.757136Z",
          "shell.execute_reply": "2024-04-06T02:57:03.749451Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_mislabeled = train.groupby(['text']).nunique().sort_values(by='target', ascending=False)\n",
        "df_mislabeled = df_mislabeled[df_mislabeled['target'] > 1]['target']\n",
        "df_mislabeled.index.tolist()"
      ],
      "metadata": {
        "id": "z6kSZcMSm6SX",
        "outputId": "eb6bf1a3-ab6b-4fac-e7b1-5fddd9c77dbc",
        "execution": {
          "iopub.status.busy": "2024-04-06T02:57:03.7529Z",
          "iopub.execute_input": "2024-04-06T02:57:03.753298Z",
          "iopub.status.idle": "2024-04-06T02:57:03.807042Z",
          "shell.execute_reply.started": "2024-04-06T02:57:03.753258Z",
          "shell.execute_reply": "2024-04-06T02:57:03.80585Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 10,
          "output_type": "execute_result",
          "data": {
            "text/plain": "[\"A look at state actions a year after Ferguson ' s upheaval \",\n 'Governor weighs parole for California school bus hijacker  ',\n 'Detonate  ( feat .  M . O . P .  )  by Apollo Brown ',\n 'Detonation fashionable mountaineering electronic watch water - resistant couples leisure tab  ',\n ' # world FedEx no longer to transport bioterror germs in wake of anthrax lab mishaps  ',\n 'I liked a  @ YouTube video from  @ Justin Stuart  GUN RANGE MAYHEM ! ',\n 'I Pledge Allegiance To The P . O . P . E .  And The Burning Buildings of Epic City .   ?  ?  ?  ?  ?  ? ',\n 'Spot Flood Combo 53 inch 300W Curved Cree LED Work Light Bar 4X4 Offroad Fog Lamp  -  Full re  ',\n 'Kosciusko police investigating pedestrian fatality hit by a train Thursday ',\n 'STAR WARS POWER OF THE JEDI COLLECTION 1 BATTLE DROID HASBRO  -  Full read by eBay  ',\n 'Caution :  breathing may be hazardous to your health . ',\n 'Who is bringing the tornadoes and floods .  Who is bringing the climate change .  God is after America He is plaguing her\\n \\n # FARRAKHAN  # QUOTE',\n 'Hollywood Movie About Trapped Miners Released in Chile ',\n 'Why are you deluged with low self - image ?  Take the quiz :   ',\n 'Do you feel like you are sinking in low self - image ?  Take the quiz :   ',\n 'that horrible sinking feeling when you have been at home on your phone for a while and you realise its been on 3G this whole time',\n 'RT Not Explained :  The only known image of infamous hijacker D . B .  Cooper .  ',\n 'To fight bioterrorism sir . ',\n 'U . S National Park Services Tonto National Forest :  Stop the Annihilation of the Salt River Wild Horse .  .  .   via  @ Change',\n 'Texas Seeks Comment on Rules for Changes to Windstorm Insurer ',\n 'Enugu Government to demolish illegal structures at International Conference Centre ',\n 'CLEARED : incident with injury : I - 495  inner loop Exit 31  -  MD 97 / Georgia Ave Silver Spring',\n 'The Dress Memes Have Officially Exploded On The Internet ',\n 'Sassy city girl country hunk stranded in Smoky Mountain snowstorm  # AoMS   # I book love  # book boost',\n 'Bayelsa poll :  Tension in Bayelsa as Patience Jonathan plans to hijack APC PDP :  Plans by former First Lady and .  .  .  ',\n 'In  # islam saving a person is equal in reward to saving all humans !  Islam is the opposite of terrorism ! ',\n 'Petition  |  Heartless owner that whipped horse until it collapsed is told he can KEEP his animal !  Act Now !  ',\n 'Watch Sarah Palin OBLITERATE Planned Parenthood For Targeting Minority Women !   BB4SP ',\n 'Fire hazard associated with installation of non - compliant external cladding on   -  By  @ www . cbp lawyers',\n 'Angry Woman Openly Accuses NEMA Of Stealing Relief Materials Meant For Internally Displaced People  :  An angry Internally Displaced wom .  .  .  ',\n 'Ashes 2015 :  Australias collapse at Trent Bridge among worst in history :  England bundled out Australia for 60  .  .  .  ',\n ' # food scare  # offers to go  # Nestle India slips into loss after  # Magginoodle  # ban unsafe and hazardous for  # human consumption',\n 'like for the music video I want some real action shit like burning buildings and police chases not some weak ben winston shit',\n 'Pandemonium In Aba As Woman Delivers Baby Without Face  ( Photos )   -  ',\n 'Mmmmmm I am burning .  .  .  .  I am burning buildings I am building .  .  .  .  Oooooohhhh oooh ooh .  .  . ',\n \"The Prophet  ( peace be upon him )  said  ' Save yourself from Hellfire even if it is by giving half a date in charity .  ' \",\n ' # hot  C - 130 specially modified to land in a stadium and rescue hostages in Iran in 1980   # prebreak  # best',\n ' # hot  Funtenna :  hijacking computers to send data as sound waves  [ Black Hat 2015 ]    # prebreak  # best',\n ' # hot  Reddit is new content policy goes into effect many horrible subreddits banned or quarantined   # prebreak  # best',\n 'FedEx no longer to transport bioterror germs in wake of anthrax lab mishaps  via  @ usatoday',\n 'FedEx no longer to transport bioterror germs in wake of anthrax lab mishaps ',\n \" @ accionempresa China ' s stock market crash this summer has sparked interest from bargain hunt .  .  .    @ gerenciatodos \",\n 'Choking Hazard Prompts Recall Of Kraft Cheese Singles ',\n ' ?  ?  New Ladies Shoulder Tote  # Handbag Faux Leather Hobo Purse Cross Body Bag  # Womens  ',\n '2pcs 18W CREE Led Work Light  Offroad Lamp Car Truck Boat Mining 4WD FLOOD BEAM  -  Full rea  ',\n \"Hollywood Movie About Trapped Miners Released in Chile :   ' The 33 '  Hollywood movie about trapped miners starring .  .  .  \",\n ' . POTUS  # Strategic Patience is a strategy for  # Genocide ;  refugees ;  IDP Internally displaced people ;  horror ;  etc .  ',\n 'World Annihilation vs Self Transformation  Aliens Attack to Exterminate Humans ',\n 'Reddit Will Now Quarantine Offensive Content ',\n 'wowo -  -  =  =  =  12000 Nigerian refugees repatriated from Cameroon',\n 'World War II book LIGHTNING JOE An Autobiography by General J .  Lawton Collins  ',\n 'Hellfire is surrounded by desires so be careful and dont let your desires control you !   # Afterlife',\n '  Cindy Noonan @ Cindy Noonan - Heartbreak in  # Baltimore  # Rioting  # YAHIstorical  # Underground Railraod',\n \" # 360WiseNews  :  China ' s Stock Market Crash :  Are There Gems In The Rubble ?  \",\n 'TWIA board approves 5 percent rate hike :  The Texas Windstorm Insurance Association  ( TWIA )  Board of Directors v .  .  .  ',\n 'Remove the  and Linkury Browser Hijacker  ',\n 'Drunk Meals 101 :  What To Cook When You are Totally Obliterated ',\n 'Survival Kit Whistle Fire Starter Wire Saw Cree Torch Emergency Blanket S knife   -  Full re  ',\n 'Reddit is new content policy goes into effect many horrible subreddits banned or quarantined  ',\n 'Heres how media in Pakistan covered the capture of terrorist Mohammed Naved ',\n 'He came to a land which was engulfed in tribal war and turned it into a land of peace i . e .  Madinah .   # Prophet Muhammad  # islam',\n 'Truth .  .  . \\n\\n # News\\n # BBC\\n # CNN\\n # Islam\\n # Truth\\n # god\\n # ISIS\\n # terrorism\\n # Quran\\n # Lies ',\n ' # Allah describes piling up  # wealth thinking it would last  # forever as the description of the people of  # Hellfire in Surah Humaza .   # Reflect',\n 'Reddit Will Now Quarantine Offensive Content  ',\n 'Hellfire !  We dont even want to think about it or mention it so let us not do anything that leads to it  # islam ! ']"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train['target_relabeled'] = train['target'].copy()\n",
        "\n",
        "train.loc[train['text'] == 'like for the music video I want some real action shit like burning buildings and police chases not some weak ben winston shit', 'target_relabeled'] = 0\n",
        "train.loc[train['text'] == 'Hellfire is surrounded by desires so be careful and donÛªt let your desires control you! #Afterlife', 'target_relabeled'] = 0\n",
        "train.loc[train['text'] == 'To fight bioterrorism sir.', 'target_relabeled'] = 0\n",
        "train.loc[train['text'] == '.POTUS #StrategicPatience is a strategy for #Genocide; refugees; IDP Internally displaced people; horror; etc. https://t.co/rqWuoy1fm4', 'target_relabeled'] = 1\n",
        "train.loc[train['text'] == 'CLEARED:incident with injury:I-495  inner loop Exit 31 - MD 97/Georgia Ave Silver Spring', 'target_relabeled'] = 1\n",
        "train.loc[train['text'] == '#foodscare #offers2go #NestleIndia slips into loss after #Magginoodle #ban unsafe and hazardous for #humanconsumption', 'target_relabeled'] = 0\n",
        "train.loc[train['text'] == 'In #islam saving a person is equal in reward to saving all humans! Islam is the opposite of terrorism!', 'target_relabeled'] = 0\n",
        "train.loc[train['text'] == 'Who is bringing the tornadoes and floods. Who is bringing the climate change. God is after America He is plaguing her\\n \\n#FARRAKHAN #QUOTE', 'target_relabeled'] = 1\n",
        "train.loc[train['text'] == 'RT NotExplained: The only known image of infamous hijacker D.B. Cooper. http://t.co/JlzK2HdeTG', 'target_relabeled'] = 1\n",
        "train.loc[train['text'] == \"Mmmmmm I'm burning.... I'm burning buildings I'm building.... Oooooohhhh oooh ooh...\", 'target_relabeled'] = 0\n",
        "train.loc[train['text'] == \"wowo--=== 12000 Nigerian refugees repatriated from Cameroon\", 'target_relabeled'] = 0\n",
        "train.loc[train['text'] == \"He came to a land which was engulfed in tribal war and turned it into a land of peace i.e. Madinah. #ProphetMuhammad #islam\", 'target_relabeled'] = 0\n",
        "train.loc[train['text'] == \"Hellfire! We donÛªt even want to think about it or mention it so letÛªs not do anything that leads to it #islam!\", 'target_relabeled'] = 0\n",
        "train.loc[train['text'] == \"The Prophet (peace be upon him) said 'Save yourself from Hellfire even if it is by giving half a date in charity.'\", 'target_relabeled'] = 0\n",
        "train.loc[train['text'] == \"Caution: breathing may be hazardous to your health.\", 'target_relabeled'] = 1\n",
        "train.loc[train['text'] == \"I Pledge Allegiance To The P.O.P.E. And The Burning Buildings of Epic City. ??????\", 'target_relabeled'] = 0\n",
        "train.loc[train['text'] == \"#Allah describes piling up #wealth thinking it would last #forever as the description of the people of #Hellfire in Surah Humaza. #Reflect\", 'target_relabeled'] = 0\n",
        "train.loc[train['text'] == \"that horrible sinking feeling when youÛªve been at home on your phone for a while and you realise its been on 3G this whole time\", 'target_relabeled'] = 0"
      ],
      "metadata": {
        "id": "m0Sw1bF5nCl5",
        "execution": {
          "iopub.status.busy": "2024-04-06T02:57:03.809051Z",
          "iopub.execute_input": "2024-04-06T02:57:03.809409Z",
          "iopub.status.idle": "2024-04-06T02:57:03.866262Z",
          "shell.execute_reply.started": "2024-04-06T02:57:03.809378Z",
          "shell.execute_reply": "2024-04-06T02:57:03.864719Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "# Load the dataset\n",
        "df = train[['text','target']]\n",
        "\n",
        "# Split the dataset into train and validation sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(df['text'], df['target'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Load the distilbert-base-uncased tokenizer and model\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=len(df['target'].unique()))\n",
        "\n",
        "# Tokenize the dataset\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "# Create the Dataset objects\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = tokenizer(text, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "        return {\n",
        "            \"input_ids\": encoding[\"input_ids\"].squeeze(),\n",
        "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n",
        "            \"labels\": torch.tensor(label)\n",
        "        }\n",
        "\n",
        "train_dataset = TextDataset(X_train.tolist(), y_train.tolist())\n",
        "val_dataset = TextDataset(X_val.tolist(), y_val.tolist())\n",
        "\n",
        "# Set up the training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./distilbert-finetuned\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=1,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "# Create the Trainer and fine-tune the model\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-06T02:57:03.868219Z",
          "iopub.execute_input": "2024-04-06T02:57:03.869223Z",
          "iopub.status.idle": "2024-04-06T06:12:01.462322Z",
          "shell.execute_reply.started": "2024-04-06T02:57:03.86917Z",
          "shell.execute_reply": "2024-04-06T06:12:01.459422Z"
        },
        "trusted": true,
        "id": "MyMN23frPuiS",
        "outputId": "7ac42337-5d9f-4daa-c39a-4abf36e0c781"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmissoumleetcoding\u001b[0m (\u001b[33mmissoum\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "wandb version 0.16.6 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Tracking run with wandb version 0.16.4"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Run data is saved locally in <code>/kaggle/working/wandb/run-20240406_025714-hgoh5xi5</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Syncing run <strong><a href='https://wandb.ai/missoum/huggingface/runs/hgoh5xi5' target=\"_blank\">xindi-queen-4</a></strong> to <a href='https://wandb.ai/missoum/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View project at <a href='https://wandb.ai/missoum/huggingface' target=\"_blank\">https://wandb.ai/missoum/huggingface</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run at <a href='https://wandb.ai/missoum/huggingface/runs/hgoh5xi5' target=\"_blank\">https://wandb.ai/missoum/huggingface/runs/hgoh5xi5</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='381' max='381' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [381/381 3:13:33, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.393079</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "execution_count": 12,
          "output_type": "execute_result",
          "data": {
            "text/plain": "TrainOutput(global_step=381, training_loss=0.4373675168655676, metrics={'train_runtime': 11690.2767, 'train_samples_per_second': 0.521, 'train_steps_per_second': 0.033, 'total_flos': 806726457815040.0, 'train_loss': 0.4373675168655676, 'epoch': 1.0})"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the fine-tuned model\n",
        "model.save_pretrained(\"./distilbert-finetuned\")\n",
        "tokenizer.save_pretrained(\"./distilbert-finetuned\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-06T06:12:01.467996Z",
          "iopub.execute_input": "2024-04-06T06:12:01.468568Z",
          "iopub.status.idle": "2024-04-06T06:12:01.922624Z",
          "shell.execute_reply.started": "2024-04-06T06:12:01.468507Z",
          "shell.execute_reply": "2024-04-06T06:12:01.92099Z"
        },
        "trusted": true,
        "id": "eJk80dxVPuiT",
        "outputId": "f2270f0c-a882-41a1-a8eb-bcbb4088bbf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 13,
          "output_type": "execute_result",
          "data": {
            "text/plain": "('./distilbert-finetuned/tokenizer_config.json',\n './distilbert-finetuned/special_tokens_map.json',\n './distilbert-finetuned/vocab.txt',\n './distilbert-finetuned/added_tokens.json')"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(text):\n",
        "    encoding = tokenizer(text, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "    output = model(**encoding)\n",
        "    predicted_label = output.logits.argmax(-1).item()\n",
        "    return predicted_label"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-06T06:21:50.336133Z",
          "iopub.execute_input": "2024-04-06T06:21:50.336648Z",
          "iopub.status.idle": "2024-04-06T06:21:50.347433Z",
          "shell.execute_reply.started": "2024-04-06T06:21:50.336615Z",
          "shell.execute_reply": "2024-04-06T06:21:50.345234Z"
        },
        "trusted": true,
        "id": "sxOBlb9uPuiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test.shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-06T06:33:08.893489Z",
          "iopub.execute_input": "2024-04-06T06:33:08.893973Z",
          "iopub.status.idle": "2024-04-06T06:33:08.906769Z",
          "shell.execute_reply.started": "2024-04-06T06:33:08.893935Z",
          "shell.execute_reply": "2024-04-06T06:33:08.904628Z"
        },
        "trusted": true,
        "id": "-CcETZk-PuiT",
        "outputId": "a199add2-ae03-47ef-c964-b1b52494902a"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 62,
          "output_type": "execute_result",
          "data": {
            "text/plain": "(3263, 4)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "test_predictions=[]\n",
        "j=0\n",
        "for i in test['text']:\n",
        "    test_predictions.append(predict(i))\n",
        "    j=j+1\n",
        "    print(j)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-06T06:33:53.43853Z",
          "iopub.execute_input": "2024-04-06T06:33:53.439008Z"
        },
        "trusted": true,
        "id": "0G9l1gELPuiT",
        "outputId": "8fdab38e-9b2a-4aba-b2c4-ab4003ac0f46"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n400\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\n500\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\n600\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\n700\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\n800\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899\n900\n901\n902\n903\n904\n905\n906\n907\n908\n909\n910\n911\n912\n913\n914\n915\n916\n917\n918\n919\n920\n921\n922\n923\n924\n925\n926\n927\n928\n929\n930\n931\n932\n933\n934\n935\n936\n937\n938\n939\n940\n941\n942\n943\n944\n945\n946\n947\n948\n949\n950\n951\n952\n953\n954\n955\n956\n957\n958\n959\n960\n961\n962\n963\n964\n965\n966\n967\n968\n969\n970\n971\n972\n973\n974\n975\n976\n977\n978\n979\n980\n981\n982\n983\n984\n985\n986\n987\n988\n989\n990\n991\n992\n993\n994\n995\n996\n997\n998\n999\n1000\n1001\n1002\n1003\n1004\n1005\n1006\n1007\n1008\n1009\n1010\n1011\n1012\n1013\n1014\n1015\n1016\n1017\n1018\n1019\n1020\n1021\n1022\n1023\n1024\n1025\n1026\n1027\n1028\n1029\n1030\n1031\n1032\n1033\n1034\n1035\n1036\n1037\n1038\n1039\n1040\n1041\n1042\n1043\n1044\n1045\n1046\n1047\n1048\n1049\n1050\n1051\n1052\n1053\n1054\n1055\n1056\n1057\n1058\n1059\n1060\n1061\n1062\n1063\n1064\n1065\n1066\n1067\n1068\n1069\n1070\n1071\n1072\n1073\n1074\n1075\n1076\n1077\n1078\n1079\n1080\n1081\n1082\n1083\n1084\n1085\n1086\n1087\n1088\n1089\n1090\n1091\n1092\n1093\n1094\n1095\n1096\n1097\n1098\n1099\n1100\n1101\n1102\n1103\n1104\n1105\n1106\n1107\n1108\n1109\n1110\n1111\n1112\n1113\n1114\n1115\n1116\n1117\n1118\n1119\n1120\n1121\n1122\n1123\n1124\n1125\n1126\n1127\n1128\n1129\n1130\n1131\n1132\n1133\n1134\n1135\n1136\n1137\n1138\n1139\n1140\n1141\n1142\n1143\n1144\n1145\n1146\n1147\n1148\n1149\n1150\n1151\n1152\n1153\n1154\n1155\n1156\n1157\n1158\n1159\n1160\n1161\n1162\n1163\n1164\n1165\n1166\n1167\n1168\n1169\n1170\n1171\n1172\n1173\n1174\n1175\n1176\n1177\n1178\n1179\n1180\n1181\n1182\n1183\n1184\n1185\n1186\n1187\n1188\n1189\n1190\n1191\n1192\n1193\n1194\n1195\n1196\n1197\n1198\n1199\n1200\n1201\n1202\n1203\n1204\n1205\n1206\n1207\n1208\n1209\n1210\n1211\n1212\n1213\n1214\n1215\n1216\n1217\n1218\n1219\n1220\n1221\n1222\n1223\n1224\n1225\n1226\n1227\n1228\n1229\n1230\n1231\n1232\n1233\n1234\n1235\n1236\n1237\n1238\n1239\n1240\n1241\n1242\n1243\n1244\n1245\n1246\n1247\n1248\n1249\n1250\n1251\n1252\n1253\n1254\n1255\n1256\n1257\n1258\n1259\n1260\n1261\n1262\n1263\n1264\n1265\n1266\n1267\n1268\n1269\n1270\n1271\n1272\n1273\n1274\n1275\n1276\n1277\n1278\n1279\n1280\n1281\n1282\n1283\n1284\n1285\n1286\n1287\n1288\n1289\n1290\n1291\n1292\n1293\n1294\n1295\n1296\n1297\n1298\n1299\n1300\n1301\n1302\n1303\n1304\n1305\n1306\n1307\n1308\n1309\n1310\n1311\n1312\n1313\n1314\n1315\n1316\n1317\n1318\n1319\n1320\n1321\n1322\n1323\n1324\n1325\n1326\n1327\n1328\n1329\n1330\n1331\n1332\n1333\n1334\n1335\n1336\n1337\n1338\n1339\n1340\n1341\n1342\n1343\n1344\n1345\n1346\n1347\n1348\n1349\n1350\n1351\n1352\n1353\n1354\n1355\n1356\n1357\n1358\n1359\n1360\n1361\n1362\n1363\n1364\n1365\n1366\n1367\n1368\n1369\n1370\n1371\n1372\n1373\n1374\n1375\n1376\n1377\n1378\n1379\n1380\n1381\n1382\n1383\n1384\n1385\n1386\n1387\n1388\n1389\n1390\n1391\n1392\n1393\n1394\n1395\n1396\n1397\n1398\n1399\n1400\n1401\n1402\n1403\n1404\n1405\n1406\n1407\n1408\n1409\n1410\n1411\n1412\n1413\n1414\n1415\n1416\n1417\n1418\n1419\n1420\n1421\n1422\n1423\n1424\n1425\n1426\n1427\n1428\n1429\n1430\n1431\n1432\n1433\n1434\n1435\n1436\n1437\n1438\n1439\n1440\n1441\n1442\n1443\n1444\n1445\n1446\n1447\n1448\n1449\n1450\n1451\n1452\n1453\n1454\n1455\n1456\n1457\n1458\n1459\n1460\n1461\n1462\n1463\n1464\n1465\n1466\n1467\n1468\n1469\n1470\n1471\n1472\n1473\n1474\n1475\n1476\n1477\n1478\n1479\n1480\n1481\n1482\n1483\n1484\n1485\n1486\n1487\n1488\n1489\n1490\n1491\n1492\n1493\n1494\n1495\n1496\n1497\n1498\n1499\n1500\n1501\n1502\n1503\n1504\n1505\n1506\n1507\n1508\n1509\n1510\n1511\n1512\n1513\n1514\n1515\n1516\n1517\n1518\n1519\n1520\n1521\n1522\n1523\n1524\n1525\n1526\n1527\n1528\n1529\n1530\n1531\n1532\n1533\n1534\n1535\n1536\n1537\n1538\n1539\n1540\n1541\n1542\n1543\n1544\n1545\n1546\n1547\n1548\n1549\n1550\n1551\n1552\n1553\n1554\n1555\n1556\n1557\n1558\n1559\n1560\n1561\n1562\n1563\n1564\n1565\n1566\n1567\n1568\n1569\n1570\n1571\n1572\n1573\n1574\n1575\n1576\n1577\n1578\n1579\n1580\n1581\n1582\n1583\n1584\n1585\n1586\n1587\n1588\n1589\n1590\n1591\n1592\n1593\n1594\n1595\n1596\n1597\n1598\n1599\n1600\n1601\n1602\n1603\n1604\n1605\n1606\n1607\n1608\n1609\n1610\n1611\n1612\n1613\n1614\n1615\n1616\n1617\n1618\n1619\n1620\n1621\n1622\n1623\n1624\n1625\n1626\n1627\n1628\n1629\n1630\n1631\n1632\n1633\n1634\n1635\n1636\n1637\n1638\n1639\n1640\n1641\n1642\n1643\n1644\n1645\n1646\n1647\n1648\n1649\n1650\n1651\n1652\n1653\n1654\n1655\n1656\n1657\n1658\n1659\n1660\n1661\n1662\n1663\n1664\n1665\n1666\n1667\n1668\n1669\n1670\n1671\n1672\n1673\n1674\n1675\n1676\n1677\n1678\n1679\n1680\n1681\n1682\n1683\n1684\n1685\n1686\n1687\n1688\n1689\n1690\n1691\n1692\n1693\n1694\n1695\n1696\n1697\n1698\n1699\n1700\n1701\n1702\n1703\n1704\n1705\n1706\n1707\n1708\n1709\n1710\n1711\n1712\n1713\n1714\n1715\n1716\n1717\n1718\n1719\n1720\n1721\n1722\n1723\n1724\n1725\n1726\n1727\n1728\n1729\n1730\n1731\n1732\n1733\n1734\n1735\n1736\n1737\n1738\n1739\n1740\n1741\n1742\n1743\n1744\n1745\n1746\n1747\n1748\n1749\n1750\n1751\n1752\n1753\n1754\n1755\n1756\n1757\n1758\n1759\n1760\n1761\n1762\n1763\n1764\n1765\n1766\n1767\n1768\n1769\n1770\n1771\n1772\n1773\n1774\n1775\n1776\n1777\n1778\n1779\n1780\n1781\n1782\n1783\n1784\n1785\n1786\n1787\n1788\n1789\n1790\n1791\n1792\n1793\n1794\n1795\n1796\n1797\n1798\n1799\n1800\n1801\n1802\n1803\n1804\n1805\n1806\n1807\n1808\n1809\n1810\n1811\n1812\n1813\n1814\n1815\n1816\n1817\n1818\n1819\n1820\n1821\n1822\n1823\n1824\n1825\n1826\n1827\n1828\n1829\n1830\n1831\n1832\n1833\n1834\n1835\n1836\n1837\n1838\n1839\n1840\n1841\n1842\n1843\n1844\n1845\n1846\n1847\n1848\n1849\n1850\n1851\n1852\n1853\n1854\n1855\n1856\n1857\n1858\n1859\n1860\n1861\n1862\n1863\n1864\n1865\n1866\n1867\n1868\n1869\n1870\n1871\n1872\n1873\n1874\n1875\n1876\n1877\n1878\n1879\n1880\n1881\n1882\n1883\n1884\n1885\n1886\n1887\n1888\n1889\n1890\n1891\n1892\n1893\n1894\n1895\n1896\n1897\n1898\n1899\n1900\n1901\n1902\n1903\n1904\n1905\n1906\n1907\n1908\n1909\n1910\n1911\n1912\n1913\n1914\n1915\n1916\n1917\n1918\n1919\n1920\n1921\n1922\n1923\n1924\n1925\n1926\n1927\n1928\n1929\n1930\n1931\n1932\n1933\n1934\n1935\n1936\n1937\n1938\n1939\n1940\n1941\n1942\n1943\n1944\n1945\n1946\n1947\n1948\n1949\n1950\n1951\n1952\n1953\n1954\n1955\n1956\n1957\n1958\n1959\n1960\n1961\n1962\n1963\n1964\n1965\n1966\n1967\n1968\n1969\n1970\n1971\n1972\n1973\n1974\n1975\n1976\n1977\n1978\n1979\n1980\n1981\n1982\n1983\n1984\n1985\n1986\n1987\n1988\n1989\n1990\n1991\n1992\n1993\n1994\n1995\n1996\n1997\n1998\n1999\n2000\n2001\n2002\n2003\n2004\n2005\n2006\n2007\n2008\n2009\n2010\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n2023\n2024\n2025\n2026\n2027\n2028\n2029\n2030\n2031\n2032\n2033\n2034\n2035\n2036\n2037\n2038\n2039\n2040\n2041\n2042\n2043\n2044\n2045\n2046\n2047\n2048\n2049\n2050\n2051\n2052\n2053\n2054\n2055\n2056\n2057\n2058\n2059\n2060\n2061\n2062\n2063\n2064\n2065\n2066\n2067\n2068\n2069\n2070\n2071\n2072\n2073\n2074\n2075\n2076\n2077\n2078\n2079\n2080\n2081\n2082\n2083\n2084\n2085\n2086\n2087\n2088\n2089\n2090\n2091\n2092\n2093\n2094\n2095\n2096\n2097\n2098\n2099\n2100\n2101\n2102\n2103\n2104\n2105\n2106\n2107\n2108\n2109\n2110\n2111\n2112\n2113\n2114\n2115\n2116\n2117\n2118\n2119\n2120\n2121\n2122\n2123\n2124\n2125\n2126\n2127\n2128\n2129\n2130\n2131\n2132\n2133\n2134\n2135\n2136\n2137\n2138\n2139\n2140\n2141\n2142\n2143\n2144\n2145\n2146\n2147\n2148\n2149\n2150\n2151\n2152\n2153\n2154\n2155\n2156\n2157\n2158\n2159\n2160\n2161\n2162\n2163\n2164\n2165\n2166\n2167\n2168\n2169\n2170\n2171\n2172\n2173\n2174\n2175\n2176\n2177\n2178\n2179\n2180\n2181\n2182\n2183\n2184\n2185\n2186\n2187\n2188\n2189\n2190\n2191\n2192\n2193\n2194\n2195\n2196\n2197\n2198\n2199\n2200\n2201\n2202\n2203\n2204\n2205\n2206\n2207\n2208\n2209\n2210\n2211\n2212\n2213\n2214\n2215\n2216\n2217\n2218\n2219\n2220\n2221\n2222\n2223\n2224\n2225\n2226\n2227\n2228\n2229\n2230\n2231\n2232\n2233\n2234\n2235\n2236\n2237\n2238\n2239\n2240\n2241\n2242\n2243\n2244\n2245\n2246\n2247\n2248\n2249\n2250\n2251\n2252\n2253\n2254\n2255\n2256\n2257\n2258\n2259\n2260\n2261\n2262\n2263\n2264\n2265\n2266\n2267\n2268\n2269\n2270\n2271\n2272\n2273\n2274\n2275\n2276\n2277\n2278\n2279\n2280\n2281\n2282\n2283\n2284\n2285\n2286\n2287\n2288\n2289\n2290\n2291\n2292\n2293\n2294\n2295\n2296\n2297\n2298\n2299\n2300\n2301\n2302\n2303\n2304\n2305\n2306\n2307\n2308\n2309\n2310\n2311\n2312\n2313\n2314\n2315\n2316\n2317\n2318\n2319\n2320\n2321\n2322\n2323\n2324\n2325\n2326\n2327\n2328\n2329\n2330\n2331\n2332\n2333\n2334\n2335\n2336\n2337\n2338\n2339\n2340\n2341\n2342\n2343\n2344\n2345\n2346\n2347\n2348\n2349\n2350\n2351\n2352\n2353\n2354\n2355\n2356\n2357\n2358\n2359\n2360\n2361\n2362\n2363\n2364\n2365\n2366\n2367\n2368\n2369\n2370\n2371\n2372\n2373\n2374\n2375\n2376\n2377\n2378\n2379\n2380\n2381\n2382\n2383\n2384\n2385\n2386\n2387\n2388\n2389\n2390\n2391\n2392\n2393\n2394\n2395\n2396\n2397\n2398\n2399\n2400\n2401\n2402\n2403\n2404\n2405\n2406\n2407\n2408\n2409\n2410\n2411\n2412\n2413\n2414\n2415\n2416\n2417\n2418\n2419\n2420\n2421\n2422\n2423\n2424\n2425\n2426\n2427\n2428\n2429\n2430\n2431\n2432\n2433\n2434\n2435\n2436\n2437\n2438\n2439\n2440\n2441\n2442\n2443\n2444\n2445\n2446\n2447\n2448\n2449\n2450\n2451\n2452\n2453\n2454\n2455\n2456\n2457\n2458\n2459\n2460\n2461\n2462\n2463\n2464\n2465\n2466\n2467\n2468\n2469\n2470\n2471\n2472\n2473\n2474\n2475\n2476\n2477\n2478\n2479\n2480\n2481\n2482\n2483\n2484\n2485\n2486\n2487\n2488\n2489\n2490\n2491\n2492\n2493\n2494\n2495\n2496\n2497\n2498\n2499\n2500\n2501\n2502\n2503\n2504\n2505\n2506\n2507\n2508\n2509\n2510\n2511\n2512\n2513\n2514\n2515\n2516\n2517\n2518\n2519\n2520\n2521\n2522\n2523\n2524\n2525\n2526\n2527\n2528\n2529\n2530\n2531\n2532\n2533\n2534\n2535\n2536\n2537\n2538\n2539\n2540\n2541\n2542\n2543\n2544\n2545\n2546\n2547\n2548\n2549\n2550\n2551\n2552\n2553\n2554\n2555\n2556\n2557\n2558\n2559\n2560\n2561\n2562\n2563\n2564\n2565\n2566\n2567\n2568\n2569\n2570\n2571\n2572\n2573\n2574\n2575\n2576\n2577\n2578\n2579\n2580\n2581\n2582\n2583\n2584\n2585\n2586\n2587\n2588\n2589\n2590\n2591\n2592\n2593\n2594\n2595\n2596\n2597\n2598\n2599\n2600\n2601\n2602\n2603\n2604\n2605\n2606\n2607\n2608\n2609\n2610\n2611\n2612\n2613\n2614\n2615\n2616\n2617\n2618\n2619\n2620\n2621\n2622\n2623\n2624\n2625\n2626\n2627\n2628\n2629\n2630\n2631\n2632\n2633\n2634\n2635\n2636\n2637\n2638\n2639\n2640\n2641\n2642\n2643\n2644\n2645\n2646\n2647\n2648\n2649\n2650\n2651\n2652\n2653\n2654\n2655\n2656\n2657\n2658\n2659\n2660\n2661\n2662\n2663\n2664\n2665\n2666\n2667\n2668\n2669\n2670\n2671\n2672\n2673\n2674\n2675\n2676\n2677\n2678\n2679\n2680\n2681\n2682\n2683\n2684\n2685\n2686\n2687\n2688\n2689\n2690\n2691\n2692\n2693\n2694\n2695\n2696\n2697\n2698\n2699\n2700\n2701\n2702\n2703\n2704\n2705\n2706\n2707\n2708\n2709\n2710\n2711\n2712\n2713\n2714\n2715\n2716\n2717\n2718\n2719\n2720\n2721\n2722\n2723\n2724\n2725\n2726\n2727\n2728\n2729\n2730\n2731\n2732\n2733\n2734\n2735\n2736\n2737\n2738\n2739\n2740\n2741\n2742\n2743\n2744\n2745\n2746\n2747\n2748\n2749\n2750\n2751\n2752\n2753\n2754\n2755\n2756\n2757\n2758\n2759\n2760\n2761\n2762\n2763\n2764\n2765\n2766\n2767\n2768\n2769\n2770\n2771\n2772\n2773\n2774\n2775\n2776\n2777\n2778\n2779\n2780\n2781\n2782\n2783\n2784\n2785\n2786\n2787\n2788\n2789\n2790\n2791\n2792\n2793\n2794\n2795\n2796\n2797\n2798\n2799\n2800\n2801\n2802\n2803\n2804\n2805\n2806\n2807\n2808\n2809\n2810\n2811\n2812\n2813\n2814\n2815\n2816\n2817\n2818\n2819\n2820\n2821\n2822\n2823\n2824\n2825\n2826\n2827\n2828\n2829\n2830\n2831\n2832\n2833\n2834\n2835\n2836\n2837\n2838\n2839\n2840\n2841\n2842\n2843\n2844\n2845\n2846\n2847\n2848\n2849\n2850\n2851\n2852\n2853\n2854\n2855\n2856\n2857\n2858\n2859\n2860\n2861\n2862\n2863\n2864\n2865\n2866\n2867\n2868\n2869\n2870\n2871\n2872\n2873\n2874\n2875\n2876\n2877\n2878\n2879\n2880\n2881\n2882\n2883\n2884\n2885\n2886\n2887\n2888\n2889\n2890\n2891\n2892\n2893\n2894\n2895\n2896\n2897\n2898\n2899\n2900\n2901\n2902\n2903\n2904\n2905\n2906\n2907\n2908\n2909\n2910\n2911\n2912\n2913\n2914\n2915\n2916\n2917\n2918\n2919\n2919\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_predictions[:5]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-06T07:07:36.55908Z",
          "iopub.execute_input": "2024-04-06T07:07:36.559518Z",
          "iopub.status.idle": "2024-04-06T07:07:36.571345Z",
          "shell.execute_reply.started": "2024-04-06T07:07:36.559485Z",
          "shell.execute_reply": "2024-04-06T07:07:36.570374Z"
        },
        "trusted": true,
        "id": "ihPfyffsPuiT",
        "outputId": "d1b061a8-0d8f-40bb-b2f1-9265c20e1f7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 72,
          "output_type": "execute_result",
          "data": {
            "text/plain": "[1, 1, 1, 1, 1]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(test_predictions))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-06T07:08:21.42058Z",
          "iopub.execute_input": "2024-04-06T07:08:21.421065Z",
          "iopub.status.idle": "2024-04-06T07:08:21.431301Z",
          "shell.execute_reply.started": "2024-04-06T07:08:21.421028Z",
          "shell.execute_reply": "2024-04-06T07:08:21.430176Z"
        },
        "trusted": true,
        "id": "kU-XT4UDPuiT",
        "outputId": "f1808ac4-8f8a-4003-effa-edc4cc91c226"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "3263\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "submission = pd.DataFrame({'id': test['id'], 'target': test_predictions})\n",
        "submission.to_csv('submission.csv', index=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-06T07:08:38.098927Z",
          "iopub.execute_input": "2024-04-06T07:08:38.100243Z",
          "iopub.status.idle": "2024-04-06T07:08:38.119188Z",
          "shell.execute_reply.started": "2024-04-06T07:08:38.100191Z",
          "shell.execute_reply": "2024-04-06T07:08:38.118056Z"
        },
        "trusted": true,
        "id": "w8IBWtPUPuiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
        "# import pandas as pd\n",
        "# import torch\n",
        "# from torch.utils.data import Dataset"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-06T06:12:01.934396Z",
          "iopub.execute_input": "2024-04-06T06:12:01.934922Z",
          "iopub.status.idle": "2024-04-06T06:12:01.942767Z",
          "shell.execute_reply.started": "2024-04-06T06:12:01.934879Z",
          "shell.execute_reply": "2024-04-06T06:12:01.941734Z"
        },
        "trusted": true,
        "id": "15VLPjvEPuiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Add additional layers\n",
        "# class CustomModel(torch.nn.Module):\n",
        "#     def __init__(self, base_model, num_labels):\n",
        "#         super().__init__()\n",
        "#         self.base_model = base_model\n",
        "#         self.dropout = torch.nn.Dropout(0.1)\n",
        "#         self.classifier = torch.nn.Linear(base_model.config.hidden_size, num_labels)\n",
        "\n",
        "#     def forward(self, input_ids, attention_mask, labels=None):\n",
        "#         outputs = self.base_model(input_ids, attention_mask=attention_mask)\n",
        "#         print(outputs)\n",
        "#         pooled_output = outputs.pooler_output\n",
        "#         pooled_output = self.dropout(pooled_output)\n",
        "#         logits = self.classifier(pooled_output)\n",
        "#         return logits\n",
        "\n",
        "# # Create the Dataset objects\n",
        "# class TextDataset(Dataset):\n",
        "#     def __init__(self, texts, labels):\n",
        "#         self.texts = texts\n",
        "#         self.labels = labels\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.texts)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         text = self.texts[idx]\n",
        "#         label = self.labels[idx]\n",
        "#         encoding = tokenizer(text, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "#         return {\n",
        "#             \"input_ids\": encoding[\"input_ids\"].squeeze(),\n",
        "#             \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n",
        "#             \"labels\": torch.tensor(label)\n",
        "#         }\n",
        "\n",
        "# def tokenize_function(examples):\n",
        "#     return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-06T06:12:01.952599Z",
          "iopub.execute_input": "2024-04-06T06:12:01.952936Z",
          "iopub.status.idle": "2024-04-06T06:12:01.962297Z"
        },
        "trusted": true,
        "id": "snZfmO4UPuiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Load the dataset\n",
        "# df = train[['text','target']]\n",
        "\n",
        "# # Split the dataset into train and validation sets\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# X_train, X_val, y_train, y_val = train_test_split(df['text'], df['target'], test_size=0.2, random_state=42)\n",
        "\n",
        "# # Load the distilbert-base-uncased tokenizer and model\n",
        "# tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "# model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=len(df['target'].unique()))\n",
        "# # Freeze the DistilBERT base model\n",
        "# for param in model.base_model.parameters():\n",
        "#     param.requires_grad = False\n",
        "# # Tokenize the dataset\n",
        "\n",
        "# train_dataset = TextDataset(X_train.tolist(), y_train.tolist())\n",
        "# val_dataset = TextDataset(X_val.tolist(), y_val.tolist())\n",
        "\n",
        "# # Set up the training arguments\n",
        "# training_args = TrainingArguments(\n",
        "#     output_dir=\"./distilbert-finetuned\",\n",
        "#     evaluation_strategy=\"epoch\",\n",
        "#     learning_rate=1e-5,\n",
        "#     per_device_train_batch_size=32,\n",
        "#     per_device_eval_batch_size=32,\n",
        "#     num_train_epochs=3,\n",
        "#     weight_decay=0.01,\n",
        "# )\n",
        "\n",
        "# custom_model = CustomModel(model.base_model, len(df['target'].unique()))\n",
        "\n",
        "# # Create the Trainer and fine-tune the model\n",
        "# trainer = Trainer(\n",
        "#     model=custom_model,\n",
        "#     args=training_args,\n",
        "#     train_dataset=train_dataset,\n",
        "#     eval_dataset=val_dataset,\n",
        "# )\n",
        "\n",
        "# trainer.train()"
      ],
      "metadata": {
        "id": "pSKGP_xbzfda",
        "execution": {
          "iopub.status.busy": "2024-04-06T06:12:01.964277Z",
          "iopub.execute_input": "2024-04-06T06:12:01.964674Z",
          "iopub.status.idle": "2024-04-06T06:12:01.975957Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Save the fine-tuned model\n",
        "# custom_model.save_pretrained(\"/kaggle/working//distilbert-finetuned\")\n",
        "# tokenizer.save_pretrained(\"/kaggle/working//distilbert-finetuned\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-06T06:12:01.977413Z",
          "iopub.execute_input": "2024-04-06T06:12:01.977855Z",
          "iopub.status.idle": "2024-04-06T06:12:01.993674Z"
        },
        "trusted": true,
        "id": "oiHGzpNjPuiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import AutoModelForSequenceClassification, AutoTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "# import torch.nn as nn\n",
        "# import torch\n",
        "# from torch.utils.data import DataLoader\n",
        "# # Load the pre-trained BERT model and tokenizer\n",
        "# model1 = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "# tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased',num_labels=2)\n",
        "\n",
        "\n",
        "# # Freeze the BERT layers (optional)\n",
        "# for param in model1.bert.parameters():\n",
        "#     param.requires_grad = False"
      ],
      "metadata": {
        "id": "ifSeUSTKRC-z",
        "execution": {
          "iopub.status.busy": "2024-04-06T06:12:01.994982Z",
          "iopub.execute_input": "2024-04-06T06:12:01.995365Z",
          "iopub.status.idle": "2024-04-06T06:12:02.00737Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_data, test_data = train_test_split(train[['text','target']], test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "gOJ3_OSlKOdy",
        "execution": {
          "iopub.status.busy": "2024-04-06T06:12:02.009004Z",
          "iopub.execute_input": "2024-04-06T06:12:02.009668Z",
          "iopub.status.idle": "2024-04-06T06:12:02.02377Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X = train['text']\n",
        "# y = train['target']"
      ],
      "metadata": {
        "id": "roLohqRGNYhh",
        "execution": {
          "iopub.status.busy": "2024-04-06T06:12:02.025427Z",
          "iopub.execute_input": "2024-04-06T06:12:02.025825Z",
          "iopub.status.idle": "2024-04-06T06:12:02.035805Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def preprocess_text(text):\n",
        "#     encoded = tokenizer.encode_plus(\n",
        "#         text,\n",
        "#         add_special_tokens=True,\n",
        "#         max_length=128,\n",
        "#         return_token_type_ids=False,\n",
        "#         padding='max_length',\n",
        "#         return_attention_mask=True,\n",
        "#         return_tensors='pt',\n",
        "#     )\n",
        "#     return encoded.input_ids, encoded.attention_mask"
      ],
      "metadata": {
        "id": "ae_aYIf2OkWe",
        "execution": {
          "iopub.status.busy": "2024-04-06T06:12:02.037298Z",
          "iopub.execute_input": "2024-04-06T06:12:02.037639Z",
          "iopub.status.idle": "2024-04-06T06:12:02.049005Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input_ids=[]\n",
        "# attention_mask=[]\n",
        "# for i in X:\n",
        "#   input_ids.append(preprocess_text(i)[0].tolist())\n",
        "#   attention_mask.append(preprocess_text(i)[1].tolist())"
      ],
      "metadata": {
        "id": "pPhheF5kQiEc",
        "execution": {
          "iopub.status.busy": "2024-04-06T06:12:02.050251Z",
          "iopub.execute_input": "2024-04-06T06:12:02.050614Z",
          "iopub.status.idle": "2024-04-06T06:12:02.067854Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_inputs = torch.tensor(input_ids)\n",
        "# train_masks = torch.tensor(attention_mask)\n",
        "# train_labels = torch.tensor(y.tolist())"
      ],
      "metadata": {
        "id": "XpcBECicNrF6",
        "execution": {
          "iopub.status.busy": "2024-04-06T06:12:02.070211Z",
          "iopub.execute_input": "2024-04-06T06:12:02.071173Z",
          "iopub.status.idle": "2024-04-06T06:12:02.078097Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Preprocess the data\n",
        "# from torch.utils.data import DataLoader\n",
        "# def preprocess_data(df):\n",
        "#     input_ids = []\n",
        "#     attention_masks = []\n",
        "#     labels = []\n",
        "\n",
        "#     for _, row in df.iterrows():\n",
        "#         encoded = tokenizer.encode_plus(\n",
        "#             row['text'],\n",
        "#             add_special_tokens=True,\n",
        "#             max_length=128,\n",
        "#             pad_to_max_length=True,\n",
        "#             return_attention_mask=True,\n",
        "#             return_tensors='pt'\n",
        "#         )\n",
        "\n",
        "#         input_ids.append(encoded['input_ids'])\n",
        "#         attention_masks.append(encoded['attention_mask'])\n",
        "#         labels.append(row['target'])\n",
        "\n",
        "#     return torch.stack(input_ids), torch.stack(attention_masks), torch.tensor(labels)"
      ],
      "metadata": {
        "id": "9Zy8tAdbbGAc",
        "execution": {
          "iopub.status.busy": "2024-04-06T06:12:02.08089Z",
          "iopub.execute_input": "2024-04-06T06:12:02.081797Z",
          "iopub.status.idle": "2024-04-06T06:12:02.090308Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Prepare the datasets\n",
        "# train_input_ids, train_attention_masks, train_labels = preprocess_data(train_data)\n",
        "# test_input_ids, test_attention_masks, test_labels = preprocess_data(test_data)\n",
        "\n",
        "# train_dataset = torch.utils.data.TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
        "# test_dataset = torch.utils.data.TensorDataset(test_input_ids, test_attention_masks, test_labels)\n",
        "\n",
        "# # Set up the dataloaders\n",
        "# train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "# test_dataloader = DataLoader(test_dataset, batch_size=16)"
      ],
      "metadata": {
        "id": "bA5GrJLAaJH_",
        "execution": {
          "iopub.status.busy": "2024-04-06T06:12:02.092616Z",
          "iopub.execute_input": "2024-04-06T06:12:02.093488Z",
          "iopub.status.idle": "2024-04-06T06:12:02.103244Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Define the classification head\n",
        "# class BertClassifier(nn.Module):\n",
        "#     def __init__(self, model):\n",
        "#         super().__init__()\n",
        "#         self.bert = model.bert\n",
        "#         self.classifier = model.classifier\n",
        "\n",
        "#     def forward(self, input_ids, attention_mask):\n",
        "#         print(self.bert(input_ids, attention_mask=attention_mask))\n",
        "#         pooled_output = self.bert(input_ids, attention_mask=attention_mask)[1]\n",
        "#         logits = self.classifier(pooled_output)\n",
        "#         return logits"
      ],
      "metadata": {
        "id": "DVfUT8JkRNBz",
        "execution": {
          "iopub.status.busy": "2024-04-06T06:12:02.105036Z",
          "iopub.execute_input": "2024-04-06T06:12:02.105471Z",
          "iopub.status.idle": "2024-04-06T06:12:02.118516Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = BertClassifier(model1)\n",
        "\n",
        "# # Set up the optimizer and scheduler\n",
        "# optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "# num_epochs = 3\n",
        "# total_steps = len(train_dataloader) * num_epochs\n",
        "# scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
      ],
      "metadata": {
        "id": "YTByXuILRRT8",
        "execution": {
          "iopub.status.busy": "2024-04-06T06:12:02.120484Z",
          "iopub.execute_input": "2024-04-06T06:12:02.120986Z",
          "iopub.status.idle": "2024-04-06T06:12:02.131548Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "JrV5TfXpynDK",
        "execution": {
          "iopub.status.busy": "2024-04-06T06:12:02.133192Z",
          "iopub.execute_input": "2024-04-06T06:12:02.134078Z",
          "iopub.status.idle": "2024-04-06T06:12:02.141814Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch.nn as nn\n",
        "# from transformers import BertModel\n",
        "# class BertClassifier(nn.Module):\n",
        "#     def __init__(self, model):\n",
        "#         super().__init__()\n",
        "#         self.bert = model.bert\n",
        "#         self.classifier = model.classifier\n",
        "\n",
        "#     def forward(self, input_ids, attention_mask):\n",
        "#         outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
        "#         pooled_output = outputs[1]\n",
        "#         logits = self.classifier(pooled_output)\n",
        "#         return logits\n",
        "# # class BertClassifier(nn.Module):\n",
        "# #     def __init__(self, freeze_bert=False):\n",
        "# #         super(BertClassifier, self).__init__()\n",
        "# #         self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "# #         self.classifier = nn.Sequential(\n",
        "# #             nn.Linear(768, 2),\n",
        "# #         )\n",
        "\n",
        "# #         if freeze_bert:\n",
        "# #             for param in self.bert.parameters():\n",
        "# #                 param.requires_grad = False\n",
        "\n",
        "# #     def forward(self, input_ids, attention_mask):\n",
        "# #         # Pass the inputs through BERT\n",
        "# #         outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)[1]  # [1] is the pooled output\n",
        "\n",
        "# #         # Pass the BERT output through the classifier\n",
        "# #         logits = self.classifier(outputs)\n",
        "# #         return logits"
      ],
      "metadata": {
        "id": "elvkR6hhKHeA",
        "execution": {
          "iopub.status.busy": "2024-04-06T06:12:02.142967Z",
          "iopub.execute_input": "2024-04-06T06:12:02.143364Z",
          "iopub.status.idle": "2024-04-06T06:12:02.158007Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
        "\n",
        "# train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "# train_sampler = SequentialSampler(train_data)\n",
        "# train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=16)"
      ],
      "metadata": {
        "id": "3_Y_M0LVK4yK",
        "execution": {
          "iopub.status.busy": "2024-04-06T06:12:02.159999Z",
          "iopub.execute_input": "2024-04-06T06:12:02.160377Z",
          "iopub.status.idle": "2024-04-06T06:12:02.174386Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Train the model\n",
        "# from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "# model = BertClassifier(freeze_bert=True)\n",
        "# optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "# # scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=100, num_training_steps=len(train_dataloader)*4)"
      ],
      "metadata": {
        "id": "lPQKFPCsLNMx",
        "execution": {
          "iopub.status.busy": "2024-04-06T06:12:02.176499Z",
          "iopub.execute_input": "2024-04-06T06:12:02.177016Z",
          "iopub.status.idle": "2024-04-06T06:12:02.187817Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9UFQBfBFNQ6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch.nn.functional as F\n",
        "\n",
        "# # # Set the device (CPU or GPU)\n",
        "# # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# # model.to(device)\n",
        "\n",
        "# # Training loop\n",
        "# num_epochs = 4\n",
        "# for epoch in range(num_epochs):\n",
        "#     model.train()\n",
        "#     for input_ids, attention_mask, labels in train_dataloader:\n",
        "#       labels = batch['targets'].to(device=device)\n",
        "#       atten_mask = batch['attention_mask'].to(device=device)\n",
        "#       input_ids = batch['input_ids'].to(device=device)\n",
        "#       print(input_ids.shape)\n",
        "#       print(attention_mask)\n",
        "#       print(labels)\n",
        "#       optimizer.zero_grad()\n",
        "#       outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "#       loss = F.cross_entropy(outputs, labels)\n",
        "#       loss.backward()\n",
        "#       optimizer.step()\n",
        "#       # scheduler.step()\n",
        "\n",
        "#     # Evaluate the model\n",
        "#     model.eval()\n",
        "#     correct = 0\n",
        "#     total = 0\n",
        "#     with torch.no_grad():\n",
        "#         for batch in test_dataloader:\n",
        "#             input_ids, attention_mask, labels = batch\n",
        "#             outputs = model(input_ids, attention_mask=attention_mask)\n",
        "#             _, predicted = torch.max(outputs.data, 1)\n",
        "#             total += labels.size(0)\n",
        "#             correct += (predicted == labels).sum().item()\n",
        "\n",
        "#     print(f'Epoch [{epoch+1}/{num_epochs}], Accuracy: {100 * correct / total:.2f}%')\n",
        "\n",
        "# # Save the trained model\n",
        "# torch.save(model.state_dict(), 'model.pth')"
      ],
      "metadata": {
        "id": "kS0EAQ5XKKCi",
        "execution": {
          "iopub.status.busy": "2024-04-06T06:12:02.189156Z",
          "iopub.execute_input": "2024-04-06T06:12:02.18949Z",
          "iopub.status.idle": "2024-04-06T06:12:02.203691Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bert_preprocess = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n",
        "# bert_encoder = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\")"
      ],
      "metadata": {
        "id": "z5_UqQjk_FzF",
        "execution": {
          "iopub.status.busy": "2024-04-06T06:12:02.20605Z",
          "iopub.execute_input": "2024-04-06T06:12:02.207114Z",
          "iopub.status.idle": "2024-04-06T06:12:02.217229Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def get_sentence_embeding(sentences):\n",
        "#     preprocessed_text = bert_preprocess(sentences)\n",
        "#     return bert_encoder(preprocessed_text)['pooled_output']"
      ],
      "metadata": {
        "id": "Gcc1DmcKM078",
        "execution": {
          "iopub.status.busy": "2024-04-06T06:12:02.218835Z",
          "iopub.execute_input": "2024-04-06T06:12:02.220169Z",
          "iopub.status.idle": "2024-04-06T06:12:02.231774Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# embd_phrases = train['text'].apply(lambda x: get_sentence_embeding([x]).numpy()[0])"
      ],
      "metadata": {
        "id": "CjQjlyKPNLnQ",
        "execution": {
          "iopub.status.busy": "2024-04-06T06:12:02.233087Z",
          "iopub.execute_input": "2024-04-06T06:12:02.233464Z",
          "iopub.status.idle": "2024-04-06T06:12:02.245112Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# embd_phrases"
      ],
      "metadata": {
        "id": "ea1cAva8NX5P",
        "execution": {
          "iopub.status.busy": "2024-04-06T06:12:02.246634Z",
          "iopub.execute_input": "2024-04-06T06:12:02.247055Z",
          "iopub.status.idle": "2024-04-06T06:12:02.260277Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# class CustomDataset(Dataset):\n",
        "#     def __init__(self, encoded_data, labels):\n",
        "#         self.encoded_data = encoded_data\n",
        "#         self.labels = labels\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         item = {key: val[idx] for key, val in self.encoded_data.items()}\n",
        "#         item['labels'] = self.labels[idx]\n",
        "#         return item\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.labels)\n",
        "\n",
        "# # Preprocess the data\n",
        "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "# train_texts, train_labels = train['text'], train['target']\n",
        "# encoded_data = tokenizer.batch_encode_plus(\n",
        "#     train_texts,\n",
        "#     truncation=True,\n",
        "#     add_special_tokens=True,\n",
        "#     return_attention_mask=True,\n",
        "#     pad_to_max_length=True,\n",
        "#     max_length=256,\n",
        "#     return_tensors='pt'\n",
        "# )\n",
        "\n",
        "# dataset = CustomDataset(encoded_data, train_labels)\n",
        "\n",
        "# # Load BERT model\n",
        "# model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "\n",
        "# # Fine-tune BERT\n",
        "# train_dataloader = DataLoader(dataset, sampler=RandomSampler(dataset), batch_size=32)\n",
        "# optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "# model.train()\n",
        "# for batch in train_dataloader:\n",
        "#     optimizer.zero_grad()\n",
        "#     input_ids = batch['input_ids']\n",
        "#     attention_mask = batch['attention_mask']\n",
        "#     labels = batch['labels']\n",
        "#     outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "#     loss = outputs.loss\n",
        "#     loss.backward()\n",
        "#     optimizer.step()\n",
        "\n",
        "# # Evaluate and predict\n",
        "# model.eval()"
      ],
      "metadata": {
        "id": "SF6fF2k3uO4r",
        "execution": {
          "iopub.status.busy": "2024-04-06T06:12:02.262558Z",
          "iopub.execute_input": "2024-04-06T06:12:02.263432Z",
          "iopub.status.idle": "2024-04-06T06:12:02.273929Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "30QArV7-uVE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import tensorflow_hub as hub\n",
        "# embedding_model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")"
      ],
      "metadata": {
        "id": "L5pIe8TxhS0s",
        "execution": {
          "iopub.status.busy": "2024-04-06T06:12:02.276146Z",
          "iopub.execute_input": "2024-04-06T06:12:02.277083Z",
          "iopub.status.idle": "2024-04-06T06:12:02.289611Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def embed(input_texts):\n",
        "#     return embedding_model(input_texts)"
      ],
      "metadata": {
        "id": "k3HB8z-shSzC",
        "execution": {
          "iopub.status.busy": "2024-04-06T06:12:02.300671Z",
          "iopub.execute_input": "2024-04-06T06:12:02.301972Z",
          "iopub.status.idle": "2024-04-06T06:12:02.307755Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# text_embeddings = embed(train['text']).numpy()\n",
        "# text_embeddings"
      ],
      "metadata": {
        "id": "Coy3nHw5hb8w",
        "execution": {
          "iopub.status.busy": "2024-04-06T06:12:02.309229Z",
          "iopub.execute_input": "2024-04-06T06:12:02.309588Z",
          "iopub.status.idle": "2024-04-06T06:12:02.321447Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from tensorflow.keras.layers import Dropout, Bidirectional\n",
        "# import tensorflow as tf\n",
        "# from tensorflow.keras.models import Sequential\n",
        "# from tensorflow.keras.layers import Dense, LSTM\n",
        "# from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
      ],
      "metadata": {
        "id": "OupwtNxLkK6-",
        "execution": {
          "iopub.status.busy": "2024-04-06T06:12:02.322853Z",
          "iopub.execute_input": "2024-04-06T06:12:02.323898Z",
          "iopub.status.idle": "2024-04-06T06:12:02.336119Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X_train, X_test, y_train, y_test = train_test_split(text_embeddings, train['target'], test_size=0.2, random_state=42)\n",
        "\n",
        "# # Reshape the input data for LSTM\n",
        "# X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
        "# X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
        "\n",
        "\n",
        "# # Define the LSTM model\n",
        "# model = Sequential()\n",
        "# model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
        "# model.add(Dense(20, activation='ReLU'))\n",
        "# model.add(Dropout(0.3))\n",
        "# model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# # Compile the model\n",
        "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# # Train the model\n",
        "# early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, mode='min', restore_best_weights=True)\n",
        "# model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, mode='min')\n",
        "\n",
        "# # Include callbacks in the fit method\n",
        "# model.fit(X_train, y_train, epochs=10, batch_size=16, validation_data=(X_test, y_test), callbacks=[early_stopping, model_checkpoint])\n",
        "# # model.fit(X_train, y_train, epochs=10, batch_size=16, validation_data=(X_test, y_test))\n",
        "\n",
        "# # Evaluate the model\n",
        "# loss, accuracy = model.evaluate(X_test, y_test)\n",
        "# print(\"Test Loss:\", loss)\n",
        "# print(\"Test Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "86HEAkeqrlXJ",
        "execution": {
          "iopub.status.busy": "2024-04-06T06:12:02.337769Z",
          "iopub.execute_input": "2024-04-06T06:12:02.338798Z",
          "iopub.status.idle": "2024-04-06T06:12:02.349259Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "WdjjH5jybpOW",
        "execution": {
          "iopub.status.busy": "2024-04-06T06:12:02.351376Z",
          "iopub.execute_input": "2024-04-06T06:12:02.352424Z",
          "iopub.status.idle": "2024-04-06T06:12:02.366065Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# y_pred = model.predict(X_test)\n",
        "# print(f\"F1 Score: {f1_score(y_test, y_pred)}\")"
      ],
      "metadata": {
        "id": "j8fFs-YpboMN",
        "execution": {
          "iopub.status.busy": "2024-04-06T06:12:02.36708Z",
          "iopub.execute_input": "2024-04-06T06:12:02.367453Z",
          "iopub.status.idle": "2024-04-06T06:12:02.378754Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# text_embeddings_test = embed(test['text']).numpy()\n",
        "# text_embeddings_test"
      ],
      "metadata": {
        "id": "Mend-e-7ub0b",
        "execution": {
          "iopub.status.busy": "2024-04-06T06:12:02.380556Z",
          "iopub.execute_input": "2024-04-06T06:12:02.381844Z",
          "iopub.status.idle": "2024-04-06T06:12:02.392644Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Ensure that text_embeddings_test is a 2D array\n",
        "# text_embeddings_test = text_embeddings_test.reshape((text_embeddings_test.shape[0], -1))\n",
        "\n",
        "# # Reshape the test data to match the input shape of the LSTM model\n",
        "# text_embeddings_test = text_embeddings_test.reshape((text_embeddings_test.shape[0], 1, text_embeddings_test.shape[1]))\n",
        "\n",
        "# # Make predictions on the test data\n",
        "# test_predictions = model.predict(text_embeddings_test)"
      ],
      "metadata": {
        "id": "-fIMDx0Ytjyo",
        "execution": {
          "iopub.status.busy": "2024-04-06T06:12:02.393886Z",
          "iopub.execute_input": "2024-04-06T06:12:02.394462Z",
          "iopub.status.idle": "2024-04-06T06:12:02.403991Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_predictions = model.predict(test['text'])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-06T06:19:08.652871Z",
          "iopub.execute_input": "2024-04-06T06:19:08.653378Z",
          "iopub.status.idle": "2024-04-06T06:19:08.928195Z",
          "shell.execute_reply.started": "2024-04-06T06:19:08.653339Z",
          "shell.execute_reply": "2024-04-06T06:19:08.926347Z"
        },
        "trusted": true,
        "id": "R3mjbirFPuia",
        "outputId": "dca4ac00-063d-48d9-a35c-ea4857286aa2"
      },
      "execution_count": null,
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[57], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m t \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m(test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DistilBertForSequenceClassification' object has no attribute 'predict'"
          ],
          "ename": "AttributeError",
          "evalue": "'DistilBertForSequenceClassification' object has no attribute 'predict'",
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "submission = pd.DataFrame({'id': test['id'], 'target': test_predictions})\n",
        "submission.to_csv('submission.csv', index=False)"
      ],
      "metadata": {
        "id": "o8uh7NyAPuia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_predictions=test_predictions.T"
      ],
      "metadata": {
        "id": "YSmD4m2evQMi",
        "execution": {
          "iopub.status.busy": "2024-04-06T06:12:02.405016Z",
          "iopub.execute_input": "2024-04-06T06:12:02.405358Z",
          "iopub.status.idle": "2024-04-06T06:12:02.417899Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_predictions"
      ],
      "metadata": {
        "id": "wIsGCk5B1c6c",
        "execution": {
          "iopub.status.busy": "2024-04-06T06:12:02.419851Z",
          "iopub.execute_input": "2024-04-06T06:12:02.420359Z",
          "iopub.status.idle": "2024-04-06T06:12:02.429325Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_predictions=test_predictions.reshape(3263)\n"
      ],
      "metadata": {
        "id": "vzhGrpYh1iCi",
        "execution": {
          "iopub.status.busy": "2024-04-06T06:12:02.430736Z",
          "iopub.execute_input": "2024-04-06T06:12:02.431314Z",
          "iopub.status.idle": "2024-04-06T06:12:02.442549Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_predictions"
      ],
      "metadata": {
        "id": "r9o-gBm41jxb",
        "execution": {
          "iopub.status.busy": "2024-04-06T06:12:02.444514Z",
          "iopub.execute_input": "2024-04-06T06:12:02.445Z",
          "iopub.status.idle": "2024-04-06T06:12:02.456162Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# t=test_predictions.tolist()"
      ],
      "metadata": {
        "id": "Snhc8CdZwlqS",
        "execution": {
          "iopub.status.busy": "2024-04-06T06:12:02.458161Z",
          "iopub.execute_input": "2024-04-06T06:12:02.458522Z",
          "iopub.status.idle": "2024-04-06T06:12:02.474498Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for i in range(len(t)):\n",
        "#     if t[i] < 0.5:\n",
        "#         t[i] = 0\n",
        "#     else:\n",
        "#         t[i] = 1\n",
        "# t"
      ],
      "metadata": {
        "id": "whoHRLE4xo47",
        "execution": {
          "iopub.status.busy": "2024-04-06T06:12:02.476546Z",
          "iopub.execute_input": "2024-04-06T06:12:02.477331Z",
          "iopub.status.idle": "2024-04-06T06:12:02.485684Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_predictions"
      ],
      "metadata": {
        "id": "yc0_YWoIx4q5",
        "execution": {
          "iopub.status.busy": "2024-04-06T06:12:02.487272Z",
          "iopub.execute_input": "2024-04-06T06:12:02.487762Z",
          "iopub.status.idle": "2024-04-06T06:12:02.496172Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Predict on the test set\n",
        "# test_predictions = model.predict(embed(test['text']).numpy())\n",
        "\n",
        "Create the submission file\n",
        "submission = pd.DataFrame({'id': test['id'], 'target': t})\n",
        "submission.to_csv('submission.csv', index=False)"
      ],
      "metadata": {
        "id": "FufrcwkkbjL8",
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2024-04-06T06:12:02.497644Z",
          "iopub.execute_input": "2024-04-06T06:12:02.498123Z",
          "iopub.status.idle": "2024-04-06T06:12:02.509601Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}